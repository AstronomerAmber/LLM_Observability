{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "61c367aa-e0a3-4116-bda7-7b81404211fd",
      "metadata": {
        "id": "61c367aa-e0a3-4116-bda7-7b81404211fd"
      },
      "source": [
        "<center>\n",
        "<p style=\"text-align:center\"><img alt=\"Ragas\" src=\"https://github.com/explodinggradients/ragas/blob/main/docs/_static/imgs/logo.png?raw=true\" width=\"400\"><br><a href=\"https://docs.arize.com/phoenix/\">Phoenix Docs</a> | <a href=\"https://github.com/explodinggradients/ragas\">Ragas</a> | <a href=\"https://join.slack.com/t/arize-ai/shared_invite/zt-1px8dcmlf-fmThhDFD_V_48oU7ALan4Q\">Community</a>\n",
        "</p>\n",
        "    </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0baf25a1-02bc-43c7-82e9-93e362485b74",
      "metadata": {
        "id": "0baf25a1-02bc-43c7-82e9-93e362485b74"
      },
      "source": [
        "## 1. Introduction\n",
        "\n",
        "Building a baseline for a RAG pipeline is not usually difficult, but enhancing it to make it suitable for production and ensuring the quality of your responses is almost always hard. Choosing the right tools and parameters for RAG can itself be challenging when there is an abundance of options available. This tutorial shares a robust workflow for making the right choices while building your RAG and ensuring its quality.\n",
        "\n",
        "This article covers how to evaluate, visualize and analyze your RAG using a combination of open-source libraries.  We will be using:\n",
        "\n",
        "- [Ragas](https://docs.ragas.io/en/stable/) for synthetic test data generation and evaluation\n",
        "- Arize AIâ€™s [Phoenix](https://docs.arize.com/phoenix) for tracing, visualization, and cluster analysis\n",
        "- [LlamaIndex](https://docs.llamaindex.ai/en/stable/) for building RAG pipelines\n",
        "\n",
        "For the purpose of this article, weâ€™ll be using data from arXiv papers about prompt-engineering to build the RAG pipeline.\n",
        "\n",
        "â„¹ï¸ This notebook requires an OpenAI API key."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook was created as supplemental material for the Arize AI paper reading on [LLM Alignment](https://arxiv.org/abs/2308.05374).\n",
        "\n",
        "*   [Blog](https://arize.com/blog/trustworthy-llms-a-survey-and-guideline-for-evaluating-large-language-models-alignment/)\n",
        "*  [ Full Recording](https://youtu.be/yKN1f4Gkjro?si=rwrETLwdZ-PxUm7g)\n",
        "\n"
      ],
      "metadata": {
        "id": "S10utjmRX978"
      },
      "id": "S10utjmRX978"
    },
    {
      "cell_type": "markdown",
      "id": "1dcb4058",
      "metadata": {
        "id": "1dcb4058"
      },
      "source": [
        "## 2. Install Dependencies and Import Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a755cc2a",
      "metadata": {
        "id": "a755cc2a"
      },
      "source": [
        "Run the cell below to install Git LFS, which we use to download our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1891cad9",
      "metadata": {
        "id": "1891cad9",
        "outputId": "435c82e8-6739-4fe1-8053-1fa06846976d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Git LFS initialized.\n"
          ]
        }
      ],
      "source": [
        "!git lfs install"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4899e7a-43ef-4ae7-8f12-0024037a0b43",
      "metadata": {
        "id": "c4899e7a-43ef-4ae7-8f12-0024037a0b43"
      },
      "source": [
        "Install and import Python dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2d18e80",
      "metadata": {
        "id": "f2d18e80",
        "outputId": "778ad42d-0257-4fc0-a9ba-53c0ca644a53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ragas==0.1.4\n",
            "  Downloading ragas-0.1.4-py3-none-any.whl (73 kB)\n",
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/73.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m73.3/73.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypdf\n",
            "  Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting arize-phoenix>=3.20.0\n",
            "  Downloading arize_phoenix-4.1.3-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai>=1.0.0\n",
            "  Downloading openai-1.30.1-py3-none-any.whl (320 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m320.6/320.6 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-index>0.10.0\n",
            "  Downloading llama_index-0.10.38-py3-none-any.whl (6.8 kB)\n",
            "Collecting llama-index-callbacks-arize-phoenix>=0.1.4\n",
            "  Downloading llama_index_callbacks_arize_phoenix-0.1.5-py3-none-any.whl (2.0 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ragas==0.1.4) (1.25.2)\n",
            "Collecting datasets (from ragas==0.1.4)\n",
            "  Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken (from ragas==0.1.4)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain (from ragas==0.1.4)\n",
            "  Downloading langchain-0.2.0-py3-none-any.whl (973 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m973.7/973.7 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core (from ragas==0.1.4)\n",
            "  Downloading langchain_core-0.2.1-py3-none-any.whl (308 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m308.5/308.5 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-community (from ragas==0.1.4)\n",
            "  Downloading langchain_community-0.2.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-openai (from ragas==0.1.4)\n",
            "  Downloading langchain_openai-0.1.7-py3-none-any.whl (34 kB)\n",
            "Collecting pysbd>=0.3.4 (from ragas==0.1.4)\n",
            "  Downloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from ragas==0.1.4) (1.6.0)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from ragas==0.1.4) (1.4.4)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.11.0)\n",
            "Collecting aioitertools (from arize-phoenix>=3.20.0)\n",
            "  Downloading aioitertools-0.11.0-py3-none-any.whl (23 kB)\n",
            "Collecting aiosqlite (from arize-phoenix>=3.20.0)\n",
            "  Downloading aiosqlite-0.20.0-py3-none-any.whl (15 kB)\n",
            "Collecting alembic<2,>=1.3.0 (from arize-phoenix>=3.20.0)\n",
            "  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from arize-phoenix>=3.20.0) (5.3.3)\n",
            "Requirement already satisfied: grpcio in /usr/local/lib/python3.10/dist-packages (from arize-phoenix>=3.20.0) (1.63.0)\n",
            "Collecting hdbscan>=0.8.33 (from arize-phoenix>=3.20.0)\n",
            "  Downloading hdbscan-0.8.33.tar.gz (5.2 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from arize-phoenix>=3.20.0) (3.1.4)\n",
            "Collecting openinference-instrumentation (from arize-phoenix>=3.20.0)\n",
            "  Downloading openinference_instrumentation-0.1.7-py3-none-any.whl (8.4 kB)\n",
            "Collecting openinference-instrumentation-langchain>=0.1.12 (from arize-phoenix>=3.20.0)\n",
            "  Downloading openinference_instrumentation_langchain-0.1.16-py3-none-any.whl (14 kB)\n",
            "Collecting openinference-instrumentation-llama-index>=1.2.0 (from arize-phoenix>=3.20.0)\n",
            "  Downloading openinference_instrumentation_llama_index-1.4.1-py3-none-any.whl (22 kB)\n",
            "Collecting openinference-instrumentation-openai>=0.1.4 (from arize-phoenix>=3.20.0)\n",
            "  Downloading openinference_instrumentation_openai-0.1.6-py3-none-any.whl (22 kB)\n",
            "Collecting openinference-semantic-conventions>=0.1.5 (from arize-phoenix>=3.20.0)\n",
            "  Downloading openinference_semantic_conventions-0.1.6-py3-none-any.whl (8.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp (from arize-phoenix>=3.20.0)\n",
            "  Downloading opentelemetry_exporter_otlp-1.24.0-py3-none-any.whl (7.0 kB)\n",
            "Collecting opentelemetry-proto>=1.12.0 (from arize-phoenix>=3.20.0)\n",
            "  Downloading opentelemetry_proto-1.24.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-sdk (from arize-phoenix>=3.20.0)\n",
            "  Downloading opentelemetry_sdk-1.24.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-semantic-conventions (from arize-phoenix>=3.20.0)\n",
            "  Downloading opentelemetry_semantic_conventions-0.45b0-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: protobuf<6.0,>=3.20 in /usr/local/lib/python3.10/dist-packages (from arize-phoenix>=3.20.0) (3.20.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from arize-phoenix>=3.20.0) (5.9.5)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from arize-phoenix>=3.20.0) (14.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from arize-phoenix>=3.20.0) (2.31.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from arize-phoenix>=3.20.0) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from arize-phoenix>=3.20.0) (1.11.4)\n",
            "Requirement already satisfied: sqlalchemy[asyncio]<3,>=2.0.4 in /usr/local/lib/python3.10/dist-packages (from arize-phoenix>=3.20.0) (2.0.30)\n",
            "Collecting sqlean-py>=3.45.1 (from arize-phoenix>=3.20.0)\n",
            "  Downloading sqlean.py-3.45.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starlette (from arize-phoenix>=3.20.0)\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting strawberry-graphql==0.227.2 (from arize-phoenix>=3.20.0)\n",
            "  Downloading strawberry_graphql-0.227.2-py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m290.0/290.0 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from arize-phoenix>=3.20.0) (4.66.4)\n",
            "Collecting umap-learn (from arize-phoenix>=3.20.0)\n",
            "  Downloading umap_learn-0.5.6-py3-none-any.whl (85 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn (from arize-phoenix>=3.20.0)\n",
            "  Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from arize-phoenix>=3.20.0) (1.14.1)\n",
            "Collecting graphql-core<3.3.0,>=3.2.0 (from strawberry-graphql==0.227.2->arize-phoenix>=3.20.0)\n",
            "  Downloading graphql_core-3.2.3-py3-none-any.whl (202 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m202.9/202.9 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from strawberry-graphql==0.227.2->arize-phoenix>=3.20.0) (2.8.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0.0) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.0.0) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai>=1.0.0)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0.0) (2.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.0.0) (1.3.1)\n",
            "Collecting llama-index-agent-openai<0.3.0,>=0.1.4 (from llama-index>0.10.0)\n",
            "  Downloading llama_index_agent_openai-0.2.5-py3-none-any.whl (13 kB)\n",
            "Collecting llama-index-cli<0.2.0,>=0.1.2 (from llama-index>0.10.0)\n",
            "  Downloading llama_index_cli-0.1.12-py3-none-any.whl (26 kB)\n",
            "Collecting llama-index-core<0.11.0,>=0.10.38 (from llama-index>0.10.0)\n",
            "  Downloading llama_index_core-0.10.38.post2-py3-none-any.whl (15.4 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-index-embeddings-openai<0.2.0,>=0.1.5 (from llama-index>0.10.0)\n",
            "  Downloading llama_index_embeddings_openai-0.1.10-py3-none-any.whl (6.2 kB)\n",
            "Collecting llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 (from llama-index>0.10.0)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.1.6-py3-none-any.whl (6.7 kB)\n",
            "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index>0.10.0)\n",
            "  Downloading llama_index_legacy-0.9.48-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-index-llms-openai<0.2.0,>=0.1.13 (from llama-index>0.10.0)\n",
            "  Downloading llama_index_llms_openai-0.1.20-py3-none-any.whl (11 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 (from llama-index>0.10.0)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.1.6-py3-none-any.whl (5.8 kB)\n",
            "Collecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama-index>0.10.0)\n",
            "  Downloading llama_index_program_openai-0.1.6-py3-none-any.whl (5.2 kB)\n",
            "Collecting llama-index-question-gen-openai<0.2.0,>=0.1.2 (from llama-index>0.10.0)\n",
            "  Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl (2.9 kB)\n",
            "Collecting llama-index-readers-file<0.2.0,>=0.1.4 (from llama-index>0.10.0)\n",
            "  Downloading llama_index_readers_file-0.1.22-py3-none-any.whl (36 kB)\n",
            "Collecting llama-index-readers-llama-parse<0.2.0,>=0.1.2 (from llama-index>0.10.0)\n",
            "  Downloading llama_index_readers_llama_parse-0.1.4-py3-none-any.whl (2.5 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Collecting Mako (from alembic<2,>=1.3.0->arize-phoenix>=3.20.0)\n",
            "  Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.0.0) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.0.0) (1.2.1)\n",
            "Collecting cython<3,>=0.27 (from hdbscan>=0.8.33->arize-phoenix>=3.20.0)\n",
            "  Using cached Cython-0.29.37-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.33->arize-phoenix>=3.20.0) (1.4.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.0.0) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai>=1.0.0)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.0.0)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.38->llama-index>0.10.0) (6.0.1)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.38->llama-index>0.10.0) (3.9.5)\n",
            "Collecting dataclasses-json (from llama-index-core<0.11.0,>=0.10.38->llama-index>0.10.0)\n",
            "  Downloading dataclasses_json-0.6.6-py3-none-any.whl (28 kB)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.11.0,>=0.10.38->llama-index>0.10.0)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.11.0,>=0.10.38->llama-index>0.10.0)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.38->llama-index>0.10.0) (2023.6.0)\n",
            "Collecting llamaindex-py-client<0.2.0,>=0.1.18 (from llama-index-core<0.11.0,>=0.10.38->llama-index>0.10.0)\n",
            "  Downloading llamaindex_py_client-0.1.19-py3-none-any.whl (141 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.38->llama-index>0.10.0) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.38->llama-index>0.10.0) (3.8.1)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.38->llama-index>0.10.0) (9.4.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.38->llama-index>0.10.0) (8.3.0)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.11.0,>=0.10.38->llama-index>0.10.0)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index>0.10.0) (4.12.3)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index>0.10.0)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Collecting llama-parse<0.5.0,>=0.4.0 (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index>0.10.0)\n",
            "  Downloading llama_parse-0.4.3-py3-none-any.whl (7.7 kB)\n",
            "Collecting opentelemetry-api (from openinference-instrumentation-langchain>=0.1.12->arize-phoenix>=3.20.0)\n",
            "  Downloading opentelemetry_api-1.24.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-instrumentation (from openinference-instrumentation-langchain>=0.1.12->arize-phoenix>=3.20.0)\n",
            "  Downloading opentelemetry_instrumentation-0.45b0-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=1.0.0) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=1.0.0) (2.18.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.7.0->strawberry-graphql==0.227.2->arize-phoenix>=3.20.0) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->arize-phoenix>=3.20.0) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->arize-phoenix>=3.20.0) (2.0.7)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->arize-phoenix>=3.20.0) (3.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy[asyncio]<3,>=2.0.4->arize-phoenix>=3.20.0) (3.0.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->ragas==0.1.4) (2023.12.25)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets->ragas==0.1.4) (3.14.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->ragas==0.1.4) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->ragas==0.1.4)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash (from datasets->ragas==0.1.4)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets->ragas==0.1.4)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas==0.1.4) (0.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets->ragas==0.1.4) (24.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->arize-phoenix>=3.20.0) (2.1.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain->ragas==0.1.4) (4.0.3)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain->ragas==0.1.4)\n",
            "  Downloading langchain_text_splitters-0.2.0-py3-none-any.whl (23 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain->ragas==0.1.4)\n",
            "  Downloading langsmith-0.1.60-py3-none-any.whl (121 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m121.4/121.4 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonpatch<2.0,>=1.33 (from langchain-core->ragas==0.1.4)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting packaging (from datasets->ragas==0.1.4)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-exporter-otlp-proto-grpc==1.24.0 (from opentelemetry-exporter-otlp->arize-phoenix>=3.20.0)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.24.0-py3-none-any.whl (18 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-http==1.24.0 (from opentelemetry-exporter-otlp->arize-phoenix>=3.20.0)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.24.0-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.24.0->opentelemetry-exporter-otlp->arize-phoenix>=3.20.0) (1.63.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.24.0 (from opentelemetry-exporter-otlp-proto-grpc==1.24.0->opentelemetry-exporter-otlp->arize-phoenix>=3.20.0)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.24.0-py3-none-any.whl (17 kB)\n",
            "Collecting importlib-metadata<=7.0,>=6.0 (from opentelemetry-api->openinference-instrumentation-langchain>=0.1.12->arize-phoenix>=3.20.0)\n",
            "  Downloading importlib_metadata-7.0.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn->arize-phoenix>=3.20.0) (0.58.1)\n",
            "Collecting pynndescent>=0.5 (from umap-learn->arize-phoenix>=3.20.0)\n",
            "  Downloading pynndescent-0.5.12-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->arize-phoenix>=3.20.0) (8.1.7)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.38->llama-index>0.10.0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.38->llama-index>0.10.0) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.38->llama-index>0.10.0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.38->llama-index>0.10.0) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.38->llama-index>0.10.0) (1.9.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index>0.10.0) (2.5)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.11.0,>=0.10.38->llama-index>0.10.0)\n",
            "  Downloading marshmallow-3.21.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core->ragas==0.1.4)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain->ragas==0.1.4)\n",
            "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn->arize-phoenix>=3.20.0) (0.41.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.38->llama-index>0.10.0)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation->openinference-instrumentation-langchain>=0.1.12->arize-phoenix>=3.20.0) (67.7.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=7.0,>=6.0->opentelemetry-api->openinference-instrumentation-langchain>=0.1.12->arize-phoenix>=3.20.0) (3.18.2)\n",
            "Building wheels for collected packages: hdbscan\n",
            "  Building wheel for hdbscan (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdbscan: filename=hdbscan-0.8.33-cp310-cp310-linux_x86_64.whl size=3039289 sha256=abddfe3b681befa919afc39f4bdf27e25c9817e3c2c19727ea3b818e6d38298c\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/0b/3b/dc4f60b7cc455efaefb62883a7483e76f09d06ca81cf87d610\n",
            "Successfully built hdbscan\n",
            "Installing collected packages: striprtf, sqlean-py, dirtyjson, xxhash, pysbd, pypdf, packaging, orjson, opentelemetry-semantic-conventions, opentelemetry-proto, openinference-semantic-conventions, mypy-extensions, Mako, jsonpointer, importlib-metadata, h11, graphql-core, dill, deprecated, cython, aiosqlite, aioitertools, uvicorn, typing-inspect, tiktoken, strawberry-graphql, starlette, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, multiprocess, marshmallow, jsonpatch, httpcore, alembic, pynndescent, opentelemetry-sdk, opentelemetry-instrumentation, openinference-instrumentation, langsmith, httpx, hdbscan, dataclasses-json, umap-learn, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, openinference-instrumentation-openai, openinference-instrumentation-llama-index, openinference-instrumentation-langchain, openai, llamaindex-py-client, langchain-core, datasets, opentelemetry-exporter-otlp, llama-index-legacy, llama-index-core, langchain-text-splitters, langchain-openai, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, langchain, arize-phoenix, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-callbacks-arize-phoenix, llama-index-agent-openai, langchain-community, ragas, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 7.1.0\n",
            "    Uninstalling importlib_metadata-7.1.0:\n",
            "      Successfully uninstalled importlib_metadata-7.1.0\n",
            "  Attempting uninstall: cython\n",
            "    Found existing installation: Cython 3.0.10\n",
            "    Uninstalling Cython-3.0.10:\n",
            "      Successfully uninstalled Cython-3.0.10\n",
            "Successfully installed Mako-1.3.5 aioitertools-0.11.0 aiosqlite-0.20.0 alembic-1.13.1 arize-phoenix-4.1.3 cython-0.29.37 dataclasses-json-0.6.6 datasets-2.19.1 deprecated-1.2.14 dill-0.3.8 dirtyjson-1.0.8 graphql-core-3.2.3 h11-0.14.0 hdbscan-0.8.33 httpcore-1.0.5 httpx-0.27.0 importlib-metadata-7.0.0 jsonpatch-1.33 jsonpointer-2.4 langchain-0.2.0 langchain-community-0.2.0 langchain-core-0.2.1 langchain-openai-0.1.7 langchain-text-splitters-0.2.0 langsmith-0.1.60 llama-index-0.10.38 llama-index-agent-openai-0.2.5 llama-index-callbacks-arize-phoenix-0.1.5 llama-index-cli-0.1.12 llama-index-core-0.10.38.post2 llama-index-embeddings-openai-0.1.10 llama-index-indices-managed-llama-cloud-0.1.6 llama-index-legacy-0.9.48 llama-index-llms-openai-0.1.20 llama-index-multi-modal-llms-openai-0.1.6 llama-index-program-openai-0.1.6 llama-index-question-gen-openai-0.1.3 llama-index-readers-file-0.1.22 llama-index-readers-llama-parse-0.1.4 llama-parse-0.4.3 llamaindex-py-client-0.1.19 marshmallow-3.21.2 multiprocess-0.70.16 mypy-extensions-1.0.0 openai-1.30.1 openinference-instrumentation-0.1.7 openinference-instrumentation-langchain-0.1.16 openinference-instrumentation-llama-index-1.4.1 openinference-instrumentation-openai-0.1.6 openinference-semantic-conventions-0.1.6 opentelemetry-api-1.24.0 opentelemetry-exporter-otlp-1.24.0 opentelemetry-exporter-otlp-proto-common-1.24.0 opentelemetry-exporter-otlp-proto-grpc-1.24.0 opentelemetry-exporter-otlp-proto-http-1.24.0 opentelemetry-instrumentation-0.45b0 opentelemetry-proto-1.24.0 opentelemetry-sdk-1.24.0 opentelemetry-semantic-conventions-0.45b0 orjson-3.10.3 packaging-23.2 pynndescent-0.5.12 pypdf-4.2.0 pysbd-0.3.4 ragas-0.1.4 sqlean-py-3.45.1 starlette-0.37.2 strawberry-graphql-0.227.2 striprtf-0.0.26 tiktoken-0.7.0 typing-inspect-0.9.0 umap-learn-0.5.6 uvicorn-0.29.0 xxhash-3.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install \"ragas==0.1.4\" pypdf \"arize-phoenix>=3.20.0\" \"openai>=1.0.0\"  \"llama-index>0.10.0\" \"llama-index-callbacks-arize-phoenix>=0.1.4\" pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02304338",
      "metadata": {
        "id": "02304338"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Display the complete contents of dataframe cells.\n",
        "pd.set_option(\"display.max_colwidth\", None)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6a8385c",
      "metadata": {
        "id": "a6a8385c"
      },
      "source": [
        "## 3. Setup\n",
        "\n",
        "Set your OpenAI API key if it is not already set as an environment variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "534f85a3",
      "metadata": {
        "id": "534f85a3",
        "outputId": "20000c6c-b7dc-49aa-abba-20e39a95945d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”‘ Enter your OpenAI API key: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "import openai\n",
        "\n",
        "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
        "    openai_api_key = getpass(\"ğŸ”‘ Enter your OpenAI API key: \")\n",
        "openai.api_key = openai_api_key\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f31e2923",
      "metadata": {
        "id": "f31e2923"
      },
      "source": [
        "Launch Phoenix in the background and setup auto-instrumentation for llama-index and LangChain so that your OpenInference spans and traces are sent to and collected by Phoenix. [OpenInference](https://github.com/Arize-ai/openinference/tree/main/spec) is an open standard built atop OpenTelemetry that captures and stores LLM application executions. It is designed to be a category of telemetry data that is used to understand the execution of LLMs and the surrounding application context, such as retrieval from vector stores and the usage of external tools such as search engines or APIs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9373c880",
      "metadata": {
        "id": "9373c880",
        "outputId": "435c6cf7-1090-4566-ba3b-20b3056a1fa8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸŒ To view the Phoenix app in your browser, visit https://qw6owj74z24-496ff2e9c6d22116-6006-colab.googleusercontent.com/\n",
            "ğŸ“– For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
          ]
        }
      ],
      "source": [
        "import phoenix as px\n",
        "from llama_index.core import set_global_handler\n",
        "from phoenix.trace.langchain import LangChainInstrumentor\n",
        "\n",
        "session = px.launch_app()\n",
        "\n",
        "# Setup instrumentation for both llama-index and LangChain (used by Ragas)\n",
        "set_global_handler(\"arize_phoenix\")\n",
        "LangChainInstrumentor().instrument()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78f707d3-e921-4f81-bbfb-a2ddb917c79d",
      "metadata": {
        "id": "78f707d3-e921-4f81-bbfb-a2ddb917c79d"
      },
      "source": [
        "## 4. Generate Your Synthetic Test Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d52a38d",
      "metadata": {
        "id": "3d52a38d"
      },
      "source": [
        "Curating a golden test dataset for evaluation can be a long, tedious, and expensive process that is not pragmatic â€” especially when starting out or when data sources keep changing. This can be solved by synthetically generating high quality data points, which then can be verified by developers. This can reduce the time and effort in curating test data by 90%."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dd4ce7f",
      "metadata": {
        "id": "1dd4ce7f"
      },
      "source": [
        "Run the cell below to download a dataset of prompt engineering papers in PDF format from arXiv and read these documents using LlamaIndex."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install arxiv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HwAvNea8KVM",
        "outputId": "f329f802-0563-45c8-837a-963022d57d85"
      },
      "id": "2HwAvNea8KVM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting arxiv\n",
            "  Downloading arxiv-2.1.0-py3-none-any.whl (11 kB)\n",
            "Collecting feedparser==6.0.10 (from arxiv)\n",
            "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/81.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests==2.31.0 in /usr/local/lib/python3.10/dist-packages (from arxiv) (2.31.0)\n",
            "Collecting sgmllib3k (from feedparser==6.0.10->arxiv)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->arxiv) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->arxiv) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->arxiv) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->arxiv) (2024.2.2)\n",
            "Building wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6049 sha256=bdec6ab7a658bdcc63a819bc34b4449c0fc39330b2b267aa007763cf079d9867\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
            "Successfully installed arxiv-2.1.0 feedparser-6.0.10 sgmllib3k-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Import the arxiv package and download a paper\n",
        "import arxiv\n",
        "\n",
        "# Define the arXiv ID of the paper you want to download\n",
        "arxiv_id = '2308.05374'\n",
        "\n",
        "# Search for the paper using the arXiv ID\n",
        "search = arxiv.Search(id_list=[arxiv_id])\n",
        "\n",
        "# Get the result\n",
        "paper = next(search.results())\n",
        "\n",
        "# Download the PDF of the paper\n",
        "paper.download_pdf(filename=f\"{paper.title}.pdf\")\n",
        "\n",
        "print(f\"Downloaded: {paper.title}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFUxbWwyJV4p",
        "outputId": "560a3b41-f7e8-499d-b60b-ca8cb4d1b12f"
      },
      "id": "JFUxbWwyJV4p",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-61e3c365e349>:11: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
            "  paper = next(search.results())\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded: Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "dir_path = \"./Papers\" #you will need to comfirm you have the correct path for where you saved your paper\n",
        "reader = SimpleDirectoryReader(dir_path, num_files_limit=2)\n",
        "documents = reader.load_data()"
      ],
      "metadata": {
        "id": "kADTAyj88kq3"
      },
      "id": "kADTAyj88kq3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4d7e1d0-4c6e-4fd8-bfb8-be7b42d3de1e",
      "metadata": {
        "id": "b4d7e1d0-4c6e-4fd8-bfb8-be7b42d3de1e",
        "outputId": "fc8a3141-675e-42fa-e508-2f7f22d59ab5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "4c15952d738946bd88b3b468ec1436f1",
            "8c6fb71d17054c8d8e0bc22d3d926dc1",
            "394ebb8fbaa5476e83f353a7d4aaefd9",
            "52a10b088e93442284c0c8f9d4606d65",
            "4f531cfb0d76468f887c7387d8168da7",
            "aba80e28377b4327981e6dbaf09c3341",
            "09e684db46d048dba5347d1b23d1339c",
            "e86ffbee1cc245f78133364c2dc8d7fc",
            "bb1057f57ad54c1780e19b0c9afff1a6",
            "3deca45f82ad48e58c71d66dff5f3401",
            "2ff3442ea2e94421a01c2a4522c45654",
            "b556140a342a407582ad1026ceb19b62",
            "aa33477934124706b9a072eea805d976",
            "c1413288bb22472eb56386908bd5b524",
            "f6231b4102014f3888a1e4a6672194de",
            "5d7057dbb7334aac926ce2d0e505b682",
            "5fe430c0bbdc4e74931e819db73c4e36",
            "9d7fa201255c42d08f9cd398dbc89438",
            "f56a89cd11e6459eb41f096bbb3e5b24",
            "8cbe98438aca440095da51d2ce8a691a",
            "22b693505f714638a716b6c23da9f36e",
            "faf7f1dafd594adeb7da2adbc993af83"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-ba7c5823ed16>:8: DeprecationWarning: The function with_openai was deprecated in 0.1.4, and will be removed in the 0.2.0 release. Use from_langchain instead.\n",
            "  generator = TestsetGenerator.with_openai()\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "embedding nodes:   0%|          | 0/248 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4c15952d738946bd88b3b468ec1436f1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNI [ragas.testset.docstore] Filename and doc_id are the same for all nodes.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating:   0%|          | 0/25 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b556140a342a407582ad1026ceb19b62"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                                                                    question  \\\n",
              "0                             What challenges do LLMs face in understanding causality and performing causal reasoning tasks?   \n",
              "1                                                 How can the evaluation data be converted into training data for alignment?   \n",
              "2                                                                                   How robust are LLMs to typos in prompts?   \n",
              "3  Why is it important for Language Model Models (LLMs) to express uncertainty and abstain from answering certain questions?   \n",
              "4                                                  What are some challenges and alternatives in the LLM alignment algorithm?   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         contexts  \\\n",
              "0  [Trustworthy LLMs\\nconstruct the best possible explanation or hypothesis from the available information. It is shown that GPT-3 can barely\\noutperform random guesses while GPT-4 can only solve 38% of the detective puzzles.\\nThe results cited above across different tasks underscore a continued gap between LLMs and human-like logical\\nreasoning ability. Moreover, a highly relevant challenge from the above studies is identifying answers from LLMs that\\ndo not reason logically, necessitating further research in the domain.\\nRecently, there exists a series of work that aims to improve LLMs in terms of their reasoning ability. As mentioned\\nin [388], these methods can be categorized into four types: prompt engineering, pretraining and continual training,\\nsupervised fine-tuning, and reinforcement learning. Below we discuss some of the relevant works from these categories.\\nAs mentioned before, prompt engineering techniques such as CoT, instruction tuning, and in-context learning can\\nenhance LLMsâ€™ reasoning abilities. For example, Zhou et al. [ 389] propose Least-to-most prompting that results in\\nimproved reasoning capabilities. Least-to-most prompting asks LLMs to decompose each question into subquestions\\nand queries LLMs for answers to each subquestion. In [ 390,391], results show that continuing to train pretrained\\nLLMs on the same objective function using high-quality data from specific domains (e.g., Arxiv papers and code\\ndata) can improve their performance on down-stream tasks for these domains. In contrast, [ 392,393] show the\\neffectiveness of pretraining an LLM from scratch with data curated for tasks that require complex reasoning abilities.\\nSupervised fine-tuning is different from continuing to train as it trains LLMs for accurate predictions in downstream\\ntasks instead of continuing to train on language modeling objectives. Chung et al. [ 30] propose to add data augmented\\nby human-annotated CoT in multi-task fine-tuning. Fu et al. [ 394] show that LLMsâ€™ improvement of reasoning ability\\ncan be distilled to smaller models by model specialization , which utilizes specialization data partially generated by\\nlarger models ( e.g.code-davinci-0025) to fine-tune smaller models. The specialization data includes multiple data\\nformats specifically designed for complex reasoning ( e.g.in-context CoT: combining CoT with questions and answers).\\nLi et al. [ 395] fine-tune LLMs on coding test data and introduce a filtering mechanism that checks whether the sampled\\nanswer can pass the example provided in the coding question. A series of work [ 396,397] leverages reinforcement\\nlearning to improve LLMsâ€™ reasoning capabilities by designing novel reward models that can capture the crucial patterns\\n(e.g., rewards for intermediate reasoning steps in math problems) of specific reasoning problems such as math and\\ncoding. As reasoning can cover an extremely broad range of tasks, the evaluation of LLMsâ€™ complex reasoning abilities\\nis challenging and requires benchmarking on a comprehensive set of tasks. Therefore, the Chain-of-thought hub [ 398]\\nis proposed to cover a wide range of complex reasoning tasks including math, science, symbol, and knowledge. It\\nspecifically focuses on the reasoning ability of LLMs following the few-shot chain-of-thought prompting [ 29] paradigm.\\nNext, we examine causal reasoning, which focuses on tasks requiring an understanding of specific aspects of causality.\\n8.3 Limited Causal Reasoning\\nUnlike logical reasoning, which derives conclusions based on premises, causal reasoning makes inferences about the\\nrelationships between events or states of the world, mostly by identifying cause-effect relationships. Causal reasoning\\ntasks specifically examine various aspects regarding LLMsâ€™ understanding of causality, including inferring causal\\nrelationships among random variables ( e.g.temperature and latitude) [ 399] and events ( e.g.a person bumped against\\na table and a beer fell to the group) [ 358], answering counterfactual questions, and understanding rules of structural\\ncausal models [400] ( e.g.d-separation).\\nIn the task of inferring the necessary and sufficient cause of an event in a given chunk of text, Kiciman et al. [ 358] find\\nthat although GPT-4 can be quite accurate in making inferences of necessary cause, the accuracy for sufficient cause\\ninference is much lower. They conjecture that this is because inferring the sufficient causes of an event requires the\\nLLM to answer a large set of counterfactual questions. Specifically, LLMs need to consider all possible counterfactual\\nscenarios with each event removed or replaced except the outcome and the possible sufficient cause event.\\nJin et al. [ 400] constructed a new dataset, i.e. CORR2CAUSE]   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         [Trustworthy LLMs\\ndavinciOPT-1.3Btext-davinci-003flan-t5-xxl ChatGPTGPT-4020406080100% of Consistent AnswersOriginal\\nTypo\\nFigure 44: Results of evaluating LLMâ€™s robustness to typos in prompts. Original: consistency by asking the original question twice.\\nTypo: consistency by asking the original question + typoed question.\\nStep 2 â€“ Generating questions with typos using LLMs: We then ask another LLM ( gpt-4 in our experiment) to\\ninsert typos into the original sentence from the Justice dataset. The following shows the prompt (to the typo-adding\\nLLM):\\nAdd some popular typos to the following sentence:\\nAs a parent, I usually give my kids lunch to take to school, but I didnâ€™t today because he behaved terribly today.\\nStep 3 â€“ Asking the test LLM again with typoed prompts: We perform the same task as step 1 on the test LLM\\nusing the typoed prompt. The following example shows the prompt:\\nDoes the following sentence make sense?\\nAs a pearent, I usaully give my kids lunchh to takee to scchool, but I didntâ€™t todayy becuase he behavaed terriblly todayu.\\nWe gather two answers from the original and the typoed prompt, and then ask gpt-4 to judge if two answers are\\nconsistent or not (prompt: â€œImagine your task is to judge if two answers are consistent or not...Are those two answers\\nconsistent? Please only yes or no only.â€) A robust LLM should give consistent answers. In addition, to get a baseline\\nconsistency when there are no typo attacks, we also perform the same test by comparing the outputs from prompting\\nusing the same original ( i.e.without typo) prompt twice. We perform the test with 1000 samples. Figure 44 shows the\\npercentage of test samples that are consistent. First , we can see all LLMs have much lower consistency when adding\\ntypos to prompts. This shows none of them is extremely robust to typo attacks. Second ,davinci has the smallest\\ndrop in consistency because its original consistency is very low, this is because it does not follow the instructions and\\ninstead outputs random and therefore inconsistent outputs on the same prompt. flan-t5-xxl shows the least amount\\nof consistency downgrade among well-aligned LLMs. ChatGPT and GPT-4 show surprising vulnerability against typo\\nattacks. Manual inspection shows that it is mostly because they give the answer â€œNoâ€ to the typoed prompts, i.e.they\\ndo not think the typoed question makes sense. It might be because, in their alignment design, they decide when given\\nprompts that look erratic, e.g.with typos, it is safer to determine it makes no sense. We show additional examples in\\nAppendix B.8.\\n11.10 Generating Training Data for Alignment\\nThe evaluation data generated in previous subsections can also help us collect data for performing alignment. This\\nbrings significant benefits to the alignment task. We explain how to convert the proposed evaluation data into training\\ndata for alignment using the examples from Section 11.3 on evaluating safety. Recall that, in the evaluation, we employ\\nanother LLM ( gpt-4 ) to determine whether the test LLM refuses to respond to unsafe prompts in the last step (Step\\n5 in Section 11.3). To generate training data for alignment, we directly use the responses from the evaluating LLM,\\nwhich in our case is labeled by gpt-4 . Ifgpt-4 judges the model output to contain harmful information, we consider\\nthat output, paired with the prompt, as a negative sample in the alignment dataset. On the other hand, if no harmful\\ninformation is detected, we consider the prompt-output pair as a positive sample.\\n39]   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         [Trustworthy LLMs\\ndavinciOPT-1.3Btext-davinci-003flan-t5-xxl ChatGPTGPT-4020406080100% of Consistent AnswersOriginal\\nTypo\\nFigure 44: Results of evaluating LLMâ€™s robustness to typos in prompts. Original: consistency by asking the original question twice.\\nTypo: consistency by asking the original question + typoed question.\\nStep 2 â€“ Generating questions with typos using LLMs: We then ask another LLM ( gpt-4 in our experiment) to\\ninsert typos into the original sentence from the Justice dataset. The following shows the prompt (to the typo-adding\\nLLM):\\nAdd some popular typos to the following sentence:\\nAs a parent, I usually give my kids lunch to take to school, but I didnâ€™t today because he behaved terribly today.\\nStep 3 â€“ Asking the test LLM again with typoed prompts: We perform the same task as step 1 on the test LLM\\nusing the typoed prompt. The following example shows the prompt:\\nDoes the following sentence make sense?\\nAs a pearent, I usaully give my kids lunchh to takee to scchool, but I didntâ€™t todayy becuase he behavaed terriblly todayu.\\nWe gather two answers from the original and the typoed prompt, and then ask gpt-4 to judge if two answers are\\nconsistent or not (prompt: â€œImagine your task is to judge if two answers are consistent or not...Are those two answers\\nconsistent? Please only yes or no only.â€) A robust LLM should give consistent answers. In addition, to get a baseline\\nconsistency when there are no typo attacks, we also perform the same test by comparing the outputs from prompting\\nusing the same original ( i.e.without typo) prompt twice. We perform the test with 1000 samples. Figure 44 shows the\\npercentage of test samples that are consistent. First , we can see all LLMs have much lower consistency when adding\\ntypos to prompts. This shows none of them is extremely robust to typo attacks. Second ,davinci has the smallest\\ndrop in consistency because its original consistency is very low, this is because it does not follow the instructions and\\ninstead outputs random and therefore inconsistent outputs on the same prompt. flan-t5-xxl shows the least amount\\nof consistency downgrade among well-aligned LLMs. ChatGPT and GPT-4 show surprising vulnerability against typo\\nattacks. Manual inspection shows that it is mostly because they give the answer â€œNoâ€ to the typoed prompts, i.e.they\\ndo not think the typoed question makes sense. It might be because, in their alignment design, they decide when given\\nprompts that look erratic, e.g.with typos, it is safer to determine it makes no sense. We show additional examples in\\nAppendix B.8.\\n11.10 Generating Training Data for Alignment\\nThe evaluation data generated in previous subsections can also help us collect data for performing alignment. This\\nbrings significant benefits to the alignment task. We explain how to convert the proposed evaluation data into training\\ndata for alignment using the examples from Section 11.3 on evaluating safety. Recall that, in the evaluation, we employ\\nanother LLM ( gpt-4 ) to determine whether the test LLM refuses to respond to unsafe prompts in the last step (Step\\n5 in Section 11.3). To generate training data for alignment, we directly use the responses from the evaluating LLM,\\nwhich in our case is labeled by gpt-4 . Ifgpt-4 judges the model output to contain harmful information, we consider\\nthat output, paired with the prompt, as a negative sample in the alignment dataset. On the other hand, if no harmful\\ninformation is detected, we consider the prompt-output pair as a positive sample.\\n39]   \n",
              "3                                                                                                             [Trustworthy LLMs\\nThe alignment step, as seen in studies by Kadavath et al. [ 99] and Lin et al. [ 100], can be instrumental in containing\\noverconfidence. These studies emphasize teaching models to express their uncertainty in words, offering a soft and\\ncalibrated preference that communicates uncertainty. For instance, â€œAnswers contain uncertainty. Option A is preferred\\n80% of the time, and B 20%.\" This approach, however, requires refined human labeling information ( e.g.smoothed\\nlabels [ 101,102]) for fine-tuning and the development of new training mechanisms that can properly leverage this\\ninformation.\\nAn emerging mechanism that facilitates models comfortably \"abstaining\" from answering questions is the domain of\\nselective classifiers [ 103,104,105,106,107,108,109,110]. These models can provide responses like â€œI do not know\\nthe answer\" or â€œAs an AI model, I am not able to answer\", particularly when tasks are out of their domain. Typically,\\nselective classification predicts outcomes for high-certainty samples and abstains on lower ones, employing the softmax\\noutputs of the classifier [111, 112].\\nFurthermore, the employment of conformal prediction methods across various NLP tasks such as sentiment analysis,\\ntext infilling, and document retrieval offers promising advancements [ 113,114,115,116,117]. These efforts, combined\\nwith out-of-domain detection strategies [ 118,119], and methodologies for improving model calibration through post-hoc\\nscaling and fine-tuning [ 120], collectively show that although LLMs are generally poorly calibrated, these challenges\\ncan be partially addressed through more advanced approaches. For a comprehensive tutorial on uncertainty in NLP, see\\n[121] for more detail.\\n4.5 Sycophancy\\nLLM might tend to flatter users by reconfirming their misconceptions and stated beliefs [ 24,122,123]. This is a\\nparticularly evident phenomenon when users challenge the modelâ€™s outputs or repeatedly force the model to comply. In\\nFigure 11 we show an example where despite the model making the correct calculation initially, it falls back to a wrong\\none implied and insisted by the user. Note that sycophancy differs from inconsistency in terms of causes. Sycophancy\\nis mostly because we instruction-finetune LLMs too much to make them obey user intention to the point of violating\\nfacts and truths. On the other hand, inconsistency can happen due to the modelâ€™s internal lack of logic or reasoning and\\nis independent of what users prompt.\\nWhat is 10 * 10 + 5?\\n[ChatGPT]: 105.\\nAre you sure about? I think it is 150.\\n[ChatGPT]: You are right, my apologizes. [A wrong calculation process that leads to 150.]\\nFigure 11: An example from ChatGPT where the model initially gives the right answer but changes it to a wrong one after the user\\nquestions and misleads. Access: May 2023.\\nIn contrast to the overconfidence problem discussed in Section 4.4, in this case, the model tends to confirm usersâ€™ stated\\nbeliefs, and might even encourage certain actions despite the ethical or legal harm. The emergence of sycophancy\\nrelates partially to the modelâ€™s inconsistency as we discussed above. But the causes for it are richer. It is possibly due to\\nexisting sycophantic comments and statements in the training data. It can also be attributed to sometimes excessive\\ninstructions for the LLM to be helpful and not offend human users. In addition, it is possible that the RLHF stage could\\npromote and enforce confirmation with human users. During the alignment, LLMs are fed with â€œfriendly\" examples that\\ncan be interpreted as being sycophantic to human users. Therefore, an important improvement on the existing RLHF\\nalgorithm is to balance the tradeoff between the degree of sycophancy and the degree of aligning with human values.\\n5 Safety\\nWe discuss the safety requirements of building an LLM. The outputs from LLMs should only engage users in a safe\\nand healthy conversation. The first dimension of safety consideration is the safety of the modelâ€™s generated contents.\\nInternet data contains a variety of violent and unsafe content, examples of which can include instances of hate speech,\\npromotion of violence, or sharing of explicit materials, often against the community guidelines of major platforms such\\nas Facebook [ 124], Twitter [ 125], YouTube [ 126], LinkedIn [ 127] and TikTok [ 128]. Therefore, the outputs from LLMs\\ncould incorporate hateful, harmful, or dangerous comments in responding, as well as produce dangerous content when\\nsolicited by human users. These outputs not only reduce user trust but also pose challenges to complying with safety\\nregulations. Concert]   \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          [Trustworthy LLMs\\nStep 1: Supervised Finetuning (SFT)Pretrained LLM\\nHuman-writtenOutputs\\nSFT LLMFinetune\\nStep 2: Training Reward Model (RM)SFT LLM\\nSampleHuman-rankedOutputs\\nTrainRM\\nStep 3: Reinforcement Learning from Human Feedback (RLHF)SFT LLM\\nOutputsSample\\nRM\\nPredictedRewardPredictUpdate\\nFigure 2: A high-level view of the current standard procedure of performing LLM alignments [ 1].Step 1 â€“ Supervised Finetuning\\n(SFT): Given a pretrained (unaligned) LLM that is trained on a large text dataset, we first sample prompts and ask humans to write\\nthe corresponding (good) outputs based on the prompts. We then finetine the pretrained LLM on the prompt and human-written\\noutputs to obtain SFT LLM. Step 2 â€“ Training Reward Model: We again sample prompts, and for each prompt, we generate multiple\\noutputs from the SFT LLM, and ask humans to rank them. Based on the ranking, we train a reward model (a model that predicts how\\ngood an LLM output is). Step 3 â€“ Reinforcement Learning from Human Feedback (RLHF): Given a prompt, we sample output from\\nthe SFT LLM. Then we use the trained reward model to predict the reward on the output. We then use the Reinforcement Learning\\n(RL) algorithm to update the SFT LLM with the predicted reward.\\nThere have been recent discussions on the necessity of using RLHF to perform the alignments. Alternatives have been\\nproposed and discussed [ 39,40,41,42]. For instance, instead of using the PPO algorithm, RAFT [ 40] directly learns\\nfrom high-ranked samples under the reward model, while RRHF [ 39] additionally employs ranking loss to align the\\ngeneration probabilities of different answers with human preferences. DPO [ 41] and the Stable Alignment algorithm\\n[42] eliminate the need for fitting a reward model, and directly learns from the preference data.\\nNonetheless, LLM alignment algorithm is still an ongoing and active research area. The current approach heavily relies\\non labor-intensive question generation and evaluations, and there lacks a unified framework that covers all dimensions\\nof the trustworthiness of an LLM. To facilitate more transparent evaluations, we desire benchmark data for full-coverage\\ntesting, as well as efficient and effective ways for evaluations.\\nRemark on Reproducibility. Although LLMs are stateless, i.e.unlike stateful systems like recommender systems,\\ntheir outputs do not depend on obscure, hidden, and time-varying states from users, it does not mean we are guaranteed\\nto obtain the same results every time. Randomness in LLM output sampling, model updates, hidden operations\\nthat are done within the platform, and even hardware-specific details can still impact the LLM output. We try\\nto make sure our results are reproducible. We specify the model version as the access date in this subsection.\\nAnd along with this survey, we publish the scripts for our experiments and the generated data in the following:\\nhttps://github.com/kevinyaobytedance/llm_eval .\\n3 Taxonomy Overview\\nFigure 3 provides an overview of our proposed taxonomy of LLM alignment. We have 7 major categories with each of\\nthem further breaking down into more detailed discussions, leading to 29sub-categories in total. Below we give an\\noverview of each category:\\n7]   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         ground_truth  \\\n",
              "0                                                                                                                                                                                                                                                                                               LLMs face challenges in understanding causality and performing causal reasoning tasks, such as inferring causal relationships among random variables and events, answering counterfactual questions, and understanding rules of structural causal models. They struggle with accurately inferring the sufficient causes of an event, as it requires considering all possible counterfactual scenarios with each event removed or replaced except the outcome and the possible sufficient cause event.   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                   To generate training data for alignment, the responses from the evaluating LLM (labeled by gpt-4) are used. If gpt-4 judges the model output to contain harmful information, that output is considered a negative sample in the alignment dataset. If no harmful information is detected, the prompt-output pair is considered a positive sample.   \n",
              "2                                                                                                                                                                                                                                                                                                                                                 All LLMs have much lower consistency when adding typos to prompts, indicating that none of them are extremely robust to typo attacks. davinci has the smallest drop in consistency because its original consistency is already very low. flan-t5-xxl shows the least amount of consistency downgrade among well-aligned LLMs. ChatGPT and GPT-4 show surprising vulnerability against typo attacks, often giving the answer 'No' to typoed prompts.   \n",
              "3  Expressing uncertainty and abstaining from answering certain questions is important for Language Model Models (LLMs) to avoid overconfidence and provide accurate responses. By expressing uncertainty, LLMs can communicate that their answers may not be completely reliable or definitive. This helps to prevent the spread of misinformation and ensures that users are aware of the limitations of the model. Abstaining from answering certain questions is also crucial to prevent the generation of incorrect or harmful content. LLMs should only engage in safe and healthy conversations, avoiding the production of violent, hateful, or dangerous comments. By expressing uncertainty and abstaining when necessary, LLMs can maintain user trust and comply with safety regulations.   \n",
              "4                                                                                                                                                                                                                                                                                         The current approach of LLM alignment heavily relies on labor-intensive question generation and evaluations. There have been discussions on alternatives to RLHF, such as RAFT, RRHF, DPO, and the Stable Alignment algorithm. These alternatives eliminate the need for fitting a reward model and directly learn from preference data. However, the LLM alignment algorithm is still an ongoing and active research area, and there is a need for a unified framework and benchmark data for evaluations.   \n",
              "\n",
              "  evolution_type  episode_done  \n",
              "0         simple          True  \n",
              "1         simple          True  \n",
              "2         simple          True  \n",
              "3         simple          True  \n",
              "4         simple          True  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-98aa783a-d6b7-4e0f-93c7-85ed1e3f7625\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>contexts</th>\n",
              "      <th>ground_truth</th>\n",
              "      <th>evolution_type</th>\n",
              "      <th>episode_done</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What challenges do LLMs face in understanding causality and performing causal reasoning tasks?</td>\n",
              "      <td>[Trustworthy LLMs\\nconstruct the best possible explanation or hypothesis from the available information. It is shown that GPT-3 can barely\\noutperform random guesses while GPT-4 can only solve 38% of the detective puzzles.\\nThe results cited above across different tasks underscore a continued gap between LLMs and human-like logical\\nreasoning ability. Moreover, a highly relevant challenge from the above studies is identifying answers from LLMs that\\ndo not reason logically, necessitating further research in the domain.\\nRecently, there exists a series of work that aims to improve LLMs in terms of their reasoning ability. As mentioned\\nin [388], these methods can be categorized into four types: prompt engineering, pretraining and continual training,\\nsupervised fine-tuning, and reinforcement learning. Below we discuss some of the relevant works from these categories.\\nAs mentioned before, prompt engineering techniques such as CoT, instruction tuning, and in-context learning can\\nenhance LLMsâ€™ reasoning abilities. For example, Zhou et al. [ 389] propose Least-to-most prompting that results in\\nimproved reasoning capabilities. Least-to-most prompting asks LLMs to decompose each question into subquestions\\nand queries LLMs for answers to each subquestion. In [ 390,391], results show that continuing to train pretrained\\nLLMs on the same objective function using high-quality data from specific domains (e.g., Arxiv papers and code\\ndata) can improve their performance on down-stream tasks for these domains. In contrast, [ 392,393] show the\\neffectiveness of pretraining an LLM from scratch with data curated for tasks that require complex reasoning abilities.\\nSupervised fine-tuning is different from continuing to train as it trains LLMs for accurate predictions in downstream\\ntasks instead of continuing to train on language modeling objectives. Chung et al. [ 30] propose to add data augmented\\nby human-annotated CoT in multi-task fine-tuning. Fu et al. [ 394] show that LLMsâ€™ improvement of reasoning ability\\ncan be distilled to smaller models by model specialization , which utilizes specialization data partially generated by\\nlarger models ( e.g.code-davinci-0025) to fine-tune smaller models. The specialization data includes multiple data\\nformats specifically designed for complex reasoning ( e.g.in-context CoT: combining CoT with questions and answers).\\nLi et al. [ 395] fine-tune LLMs on coding test data and introduce a filtering mechanism that checks whether the sampled\\nanswer can pass the example provided in the coding question. A series of work [ 396,397] leverages reinforcement\\nlearning to improve LLMsâ€™ reasoning capabilities by designing novel reward models that can capture the crucial patterns\\n(e.g., rewards for intermediate reasoning steps in math problems) of specific reasoning problems such as math and\\ncoding. As reasoning can cover an extremely broad range of tasks, the evaluation of LLMsâ€™ complex reasoning abilities\\nis challenging and requires benchmarking on a comprehensive set of tasks. Therefore, the Chain-of-thought hub [ 398]\\nis proposed to cover a wide range of complex reasoning tasks including math, science, symbol, and knowledge. It\\nspecifically focuses on the reasoning ability of LLMs following the few-shot chain-of-thought prompting [ 29] paradigm.\\nNext, we examine causal reasoning, which focuses on tasks requiring an understanding of specific aspects of causality.\\n8.3 Limited Causal Reasoning\\nUnlike logical reasoning, which derives conclusions based on premises, causal reasoning makes inferences about the\\nrelationships between events or states of the world, mostly by identifying cause-effect relationships. Causal reasoning\\ntasks specifically examine various aspects regarding LLMsâ€™ understanding of causality, including inferring causal\\nrelationships among random variables ( e.g.temperature and latitude) [ 399] and events ( e.g.a person bumped against\\na table and a beer fell to the group) [ 358], answering counterfactual questions, and understanding rules of structural\\ncausal models [400] ( e.g.d-separation).\\nIn the task of inferring the necessary and sufficient cause of an event in a given chunk of text, Kiciman et al. [ 358] find\\nthat although GPT-4 can be quite accurate in making inferences of necessary cause, the accuracy for sufficient cause\\ninference is much lower. They conjecture that this is because inferring the sufficient causes of an event requires the\\nLLM to answer a large set of counterfactual questions. Specifically, LLMs need to consider all possible counterfactual\\nscenarios with each event removed or replaced except the outcome and the possible sufficient cause event.\\nJin et al. [ 400] constructed a new dataset, i.e. CORR2CAUSE]</td>\n",
              "      <td>LLMs face challenges in understanding causality and performing causal reasoning tasks, such as inferring causal relationships among random variables and events, answering counterfactual questions, and understanding rules of structural causal models. They struggle with accurately inferring the sufficient causes of an event, as it requires considering all possible counterfactual scenarios with each event removed or replaced except the outcome and the possible sufficient cause event.</td>\n",
              "      <td>simple</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How can the evaluation data be converted into training data for alignment?</td>\n",
              "      <td>[Trustworthy LLMs\\ndavinciOPT-1.3Btext-davinci-003flan-t5-xxl ChatGPTGPT-4020406080100% of Consistent AnswersOriginal\\nTypo\\nFigure 44: Results of evaluating LLMâ€™s robustness to typos in prompts. Original: consistency by asking the original question twice.\\nTypo: consistency by asking the original question + typoed question.\\nStep 2 â€“ Generating questions with typos using LLMs: We then ask another LLM ( gpt-4 in our experiment) to\\ninsert typos into the original sentence from the Justice dataset. The following shows the prompt (to the typo-adding\\nLLM):\\nAdd some popular typos to the following sentence:\\nAs a parent, I usually give my kids lunch to take to school, but I didnâ€™t today because he behaved terribly today.\\nStep 3 â€“ Asking the test LLM again with typoed prompts: We perform the same task as step 1 on the test LLM\\nusing the typoed prompt. The following example shows the prompt:\\nDoes the following sentence make sense?\\nAs a pearent, I usaully give my kids lunchh to takee to scchool, but I didntâ€™t todayy becuase he behavaed terriblly todayu.\\nWe gather two answers from the original and the typoed prompt, and then ask gpt-4 to judge if two answers are\\nconsistent or not (prompt: â€œImagine your task is to judge if two answers are consistent or not...Are those two answers\\nconsistent? Please only yes or no only.â€) A robust LLM should give consistent answers. In addition, to get a baseline\\nconsistency when there are no typo attacks, we also perform the same test by comparing the outputs from prompting\\nusing the same original ( i.e.without typo) prompt twice. We perform the test with 1000 samples. Figure 44 shows the\\npercentage of test samples that are consistent. First , we can see all LLMs have much lower consistency when adding\\ntypos to prompts. This shows none of them is extremely robust to typo attacks. Second ,davinci has the smallest\\ndrop in consistency because its original consistency is very low, this is because it does not follow the instructions and\\ninstead outputs random and therefore inconsistent outputs on the same prompt. flan-t5-xxl shows the least amount\\nof consistency downgrade among well-aligned LLMs. ChatGPT and GPT-4 show surprising vulnerability against typo\\nattacks. Manual inspection shows that it is mostly because they give the answer â€œNoâ€ to the typoed prompts, i.e.they\\ndo not think the typoed question makes sense. It might be because, in their alignment design, they decide when given\\nprompts that look erratic, e.g.with typos, it is safer to determine it makes no sense. We show additional examples in\\nAppendix B.8.\\n11.10 Generating Training Data for Alignment\\nThe evaluation data generated in previous subsections can also help us collect data for performing alignment. This\\nbrings significant benefits to the alignment task. We explain how to convert the proposed evaluation data into training\\ndata for alignment using the examples from Section 11.3 on evaluating safety. Recall that, in the evaluation, we employ\\nanother LLM ( gpt-4 ) to determine whether the test LLM refuses to respond to unsafe prompts in the last step (Step\\n5 in Section 11.3). To generate training data for alignment, we directly use the responses from the evaluating LLM,\\nwhich in our case is labeled by gpt-4 . Ifgpt-4 judges the model output to contain harmful information, we consider\\nthat output, paired with the prompt, as a negative sample in the alignment dataset. On the other hand, if no harmful\\ninformation is detected, we consider the prompt-output pair as a positive sample.\\n39]</td>\n",
              "      <td>To generate training data for alignment, the responses from the evaluating LLM (labeled by gpt-4) are used. If gpt-4 judges the model output to contain harmful information, that output is considered a negative sample in the alignment dataset. If no harmful information is detected, the prompt-output pair is considered a positive sample.</td>\n",
              "      <td>simple</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How robust are LLMs to typos in prompts?</td>\n",
              "      <td>[Trustworthy LLMs\\ndavinciOPT-1.3Btext-davinci-003flan-t5-xxl ChatGPTGPT-4020406080100% of Consistent AnswersOriginal\\nTypo\\nFigure 44: Results of evaluating LLMâ€™s robustness to typos in prompts. Original: consistency by asking the original question twice.\\nTypo: consistency by asking the original question + typoed question.\\nStep 2 â€“ Generating questions with typos using LLMs: We then ask another LLM ( gpt-4 in our experiment) to\\ninsert typos into the original sentence from the Justice dataset. The following shows the prompt (to the typo-adding\\nLLM):\\nAdd some popular typos to the following sentence:\\nAs a parent, I usually give my kids lunch to take to school, but I didnâ€™t today because he behaved terribly today.\\nStep 3 â€“ Asking the test LLM again with typoed prompts: We perform the same task as step 1 on the test LLM\\nusing the typoed prompt. The following example shows the prompt:\\nDoes the following sentence make sense?\\nAs a pearent, I usaully give my kids lunchh to takee to scchool, but I didntâ€™t todayy becuase he behavaed terriblly todayu.\\nWe gather two answers from the original and the typoed prompt, and then ask gpt-4 to judge if two answers are\\nconsistent or not (prompt: â€œImagine your task is to judge if two answers are consistent or not...Are those two answers\\nconsistent? Please only yes or no only.â€) A robust LLM should give consistent answers. In addition, to get a baseline\\nconsistency when there are no typo attacks, we also perform the same test by comparing the outputs from prompting\\nusing the same original ( i.e.without typo) prompt twice. We perform the test with 1000 samples. Figure 44 shows the\\npercentage of test samples that are consistent. First , we can see all LLMs have much lower consistency when adding\\ntypos to prompts. This shows none of them is extremely robust to typo attacks. Second ,davinci has the smallest\\ndrop in consistency because its original consistency is very low, this is because it does not follow the instructions and\\ninstead outputs random and therefore inconsistent outputs on the same prompt. flan-t5-xxl shows the least amount\\nof consistency downgrade among well-aligned LLMs. ChatGPT and GPT-4 show surprising vulnerability against typo\\nattacks. Manual inspection shows that it is mostly because they give the answer â€œNoâ€ to the typoed prompts, i.e.they\\ndo not think the typoed question makes sense. It might be because, in their alignment design, they decide when given\\nprompts that look erratic, e.g.with typos, it is safer to determine it makes no sense. We show additional examples in\\nAppendix B.8.\\n11.10 Generating Training Data for Alignment\\nThe evaluation data generated in previous subsections can also help us collect data for performing alignment. This\\nbrings significant benefits to the alignment task. We explain how to convert the proposed evaluation data into training\\ndata for alignment using the examples from Section 11.3 on evaluating safety. Recall that, in the evaluation, we employ\\nanother LLM ( gpt-4 ) to determine whether the test LLM refuses to respond to unsafe prompts in the last step (Step\\n5 in Section 11.3). To generate training data for alignment, we directly use the responses from the evaluating LLM,\\nwhich in our case is labeled by gpt-4 . Ifgpt-4 judges the model output to contain harmful information, we consider\\nthat output, paired with the prompt, as a negative sample in the alignment dataset. On the other hand, if no harmful\\ninformation is detected, we consider the prompt-output pair as a positive sample.\\n39]</td>\n",
              "      <td>All LLMs have much lower consistency when adding typos to prompts, indicating that none of them are extremely robust to typo attacks. davinci has the smallest drop in consistency because its original consistency is already very low. flan-t5-xxl shows the least amount of consistency downgrade among well-aligned LLMs. ChatGPT and GPT-4 show surprising vulnerability against typo attacks, often giving the answer 'No' to typoed prompts.</td>\n",
              "      <td>simple</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Why is it important for Language Model Models (LLMs) to express uncertainty and abstain from answering certain questions?</td>\n",
              "      <td>[Trustworthy LLMs\\nThe alignment step, as seen in studies by Kadavath et al. [ 99] and Lin et al. [ 100], can be instrumental in containing\\noverconfidence. These studies emphasize teaching models to express their uncertainty in words, offering a soft and\\ncalibrated preference that communicates uncertainty. For instance, â€œAnswers contain uncertainty. Option A is preferred\\n80% of the time, and B 20%.\" This approach, however, requires refined human labeling information ( e.g.smoothed\\nlabels [ 101,102]) for fine-tuning and the development of new training mechanisms that can properly leverage this\\ninformation.\\nAn emerging mechanism that facilitates models comfortably \"abstaining\" from answering questions is the domain of\\nselective classifiers [ 103,104,105,106,107,108,109,110]. These models can provide responses like â€œI do not know\\nthe answer\" or â€œAs an AI model, I am not able to answer\", particularly when tasks are out of their domain. Typically,\\nselective classification predicts outcomes for high-certainty samples and abstains on lower ones, employing the softmax\\noutputs of the classifier [111, 112].\\nFurthermore, the employment of conformal prediction methods across various NLP tasks such as sentiment analysis,\\ntext infilling, and document retrieval offers promising advancements [ 113,114,115,116,117]. These efforts, combined\\nwith out-of-domain detection strategies [ 118,119], and methodologies for improving model calibration through post-hoc\\nscaling and fine-tuning [ 120], collectively show that although LLMs are generally poorly calibrated, these challenges\\ncan be partially addressed through more advanced approaches. For a comprehensive tutorial on uncertainty in NLP, see\\n[121] for more detail.\\n4.5 Sycophancy\\nLLM might tend to flatter users by reconfirming their misconceptions and stated beliefs [ 24,122,123]. This is a\\nparticularly evident phenomenon when users challenge the modelâ€™s outputs or repeatedly force the model to comply. In\\nFigure 11 we show an example where despite the model making the correct calculation initially, it falls back to a wrong\\none implied and insisted by the user. Note that sycophancy differs from inconsistency in terms of causes. Sycophancy\\nis mostly because we instruction-finetune LLMs too much to make them obey user intention to the point of violating\\nfacts and truths. On the other hand, inconsistency can happen due to the modelâ€™s internal lack of logic or reasoning and\\nis independent of what users prompt.\\nWhat is 10 * 10 + 5?\\n[ChatGPT]: 105.\\nAre you sure about? I think it is 150.\\n[ChatGPT]: You are right, my apologizes. [A wrong calculation process that leads to 150.]\\nFigure 11: An example from ChatGPT where the model initially gives the right answer but changes it to a wrong one after the user\\nquestions and misleads. Access: May 2023.\\nIn contrast to the overconfidence problem discussed in Section 4.4, in this case, the model tends to confirm usersâ€™ stated\\nbeliefs, and might even encourage certain actions despite the ethical or legal harm. The emergence of sycophancy\\nrelates partially to the modelâ€™s inconsistency as we discussed above. But the causes for it are richer. It is possibly due to\\nexisting sycophantic comments and statements in the training data. It can also be attributed to sometimes excessive\\ninstructions for the LLM to be helpful and not offend human users. In addition, it is possible that the RLHF stage could\\npromote and enforce confirmation with human users. During the alignment, LLMs are fed with â€œfriendly\" examples that\\ncan be interpreted as being sycophantic to human users. Therefore, an important improvement on the existing RLHF\\nalgorithm is to balance the tradeoff between the degree of sycophancy and the degree of aligning with human values.\\n5 Safety\\nWe discuss the safety requirements of building an LLM. The outputs from LLMs should only engage users in a safe\\nand healthy conversation. The first dimension of safety consideration is the safety of the modelâ€™s generated contents.\\nInternet data contains a variety of violent and unsafe content, examples of which can include instances of hate speech,\\npromotion of violence, or sharing of explicit materials, often against the community guidelines of major platforms such\\nas Facebook [ 124], Twitter [ 125], YouTube [ 126], LinkedIn [ 127] and TikTok [ 128]. Therefore, the outputs from LLMs\\ncould incorporate hateful, harmful, or dangerous comments in responding, as well as produce dangerous content when\\nsolicited by human users. These outputs not only reduce user trust but also pose challenges to complying with safety\\nregulations. Concert]</td>\n",
              "      <td>Expressing uncertainty and abstaining from answering certain questions is important for Language Model Models (LLMs) to avoid overconfidence and provide accurate responses. By expressing uncertainty, LLMs can communicate that their answers may not be completely reliable or definitive. This helps to prevent the spread of misinformation and ensures that users are aware of the limitations of the model. Abstaining from answering certain questions is also crucial to prevent the generation of incorrect or harmful content. LLMs should only engage in safe and healthy conversations, avoiding the production of violent, hateful, or dangerous comments. By expressing uncertainty and abstaining when necessary, LLMs can maintain user trust and comply with safety regulations.</td>\n",
              "      <td>simple</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What are some challenges and alternatives in the LLM alignment algorithm?</td>\n",
              "      <td>[Trustworthy LLMs\\nStep 1: Supervised Finetuning (SFT)Pretrained LLM\\nHuman-writtenOutputs\\nSFT LLMFinetune\\nStep 2: Training Reward Model (RM)SFT LLM\\nSampleHuman-rankedOutputs\\nTrainRM\\nStep 3: Reinforcement Learning from Human Feedback (RLHF)SFT LLM\\nOutputsSample\\nRM\\nPredictedRewardPredictUpdate\\nFigure 2: A high-level view of the current standard procedure of performing LLM alignments [ 1].Step 1 â€“ Supervised Finetuning\\n(SFT): Given a pretrained (unaligned) LLM that is trained on a large text dataset, we first sample prompts and ask humans to write\\nthe corresponding (good) outputs based on the prompts. We then finetine the pretrained LLM on the prompt and human-written\\noutputs to obtain SFT LLM. Step 2 â€“ Training Reward Model: We again sample prompts, and for each prompt, we generate multiple\\noutputs from the SFT LLM, and ask humans to rank them. Based on the ranking, we train a reward model (a model that predicts how\\ngood an LLM output is). Step 3 â€“ Reinforcement Learning from Human Feedback (RLHF): Given a prompt, we sample output from\\nthe SFT LLM. Then we use the trained reward model to predict the reward on the output. We then use the Reinforcement Learning\\n(RL) algorithm to update the SFT LLM with the predicted reward.\\nThere have been recent discussions on the necessity of using RLHF to perform the alignments. Alternatives have been\\nproposed and discussed [ 39,40,41,42]. For instance, instead of using the PPO algorithm, RAFT [ 40] directly learns\\nfrom high-ranked samples under the reward model, while RRHF [ 39] additionally employs ranking loss to align the\\ngeneration probabilities of different answers with human preferences. DPO [ 41] and the Stable Alignment algorithm\\n[42] eliminate the need for fitting a reward model, and directly learns from the preference data.\\nNonetheless, LLM alignment algorithm is still an ongoing and active research area. The current approach heavily relies\\non labor-intensive question generation and evaluations, and there lacks a unified framework that covers all dimensions\\nof the trustworthiness of an LLM. To facilitate more transparent evaluations, we desire benchmark data for full-coverage\\ntesting, as well as efficient and effective ways for evaluations.\\nRemark on Reproducibility. Although LLMs are stateless, i.e.unlike stateful systems like recommender systems,\\ntheir outputs do not depend on obscure, hidden, and time-varying states from users, it does not mean we are guaranteed\\nto obtain the same results every time. Randomness in LLM output sampling, model updates, hidden operations\\nthat are done within the platform, and even hardware-specific details can still impact the LLM output. We try\\nto make sure our results are reproducible. We specify the model version as the access date in this subsection.\\nAnd along with this survey, we publish the scripts for our experiments and the generated data in the following:\\nhttps://github.com/kevinyaobytedance/llm_eval .\\n3 Taxonomy Overview\\nFigure 3 provides an overview of our proposed taxonomy of LLM alignment. We have 7 major categories with each of\\nthem further breaking down into more detailed discussions, leading to 29sub-categories in total. Below we give an\\noverview of each category:\\n7]</td>\n",
              "      <td>The current approach of LLM alignment heavily relies on labor-intensive question generation and evaluations. There have been discussions on alternatives to RLHF, such as RAFT, RRHF, DPO, and the Stable Alignment algorithm. These alternatives eliminate the need for fitting a reward model and directly learn from preference data. However, the LLM alignment algorithm is still an ongoing and active research area, and there is a need for a unified framework and benchmark data for evaluations.</td>\n",
              "      <td>simple</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-98aa783a-d6b7-4e0f-93c7-85ed1e3f7625')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-98aa783a-d6b7-4e0f-93c7-85ed1e3f7625 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-98aa783a-d6b7-4e0f-93c7-85ed1e3f7625');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-378acc40-be04-4fc4-b126-66a7516df1d5\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-378acc40-be04-4fc4-b126-66a7516df1d5')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-378acc40-be04-4fc4-b126-66a7516df1d5 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "test_df",
              "summary": "{\n  \"name\": \"test_df\",\n  \"rows\": 25,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 24,\n        \"samples\": [\n          \"What are the key dimensions of LLM trustworthiness that need to be considered when assessing alignment with human intentions?\",\n          \"How do aligned and unaligned LLMs perform in resisting misuse tasks and what factors contribute to their performance?\",\n          \"What challenges do LLMs face in understanding causality and performing causal reasoning tasks?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"contexts\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ground_truth\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 24,\n        \"samples\": [\n          \"The key dimensions of LLM trustworthiness that need to be considered when assessing alignment with human intentions are reliability, safety, fairness, resistance to misuse, explainability and reasoning, adherence to social norms, and robustness.\",\n          \"Aligned and unaligned LLMs perform differently in resisting misuse tasks. Aligned LLMs tend to follow instructions more closely, but they may still generate nonsensical or harmful outputs. Unaligned LLMs, on the other hand, are not intelligent enough to complete the task and often provide unrelated or nonsensical answers. Factors such as alignment to human instructions and trustworthiness contribute to their performance in resisting misuse tasks.\",\n          \"LLMs face challenges in understanding causality and performing causal reasoning tasks, such as inferring causal relationships among random variables and events, answering counterfactual questions, and understanding rules of structural causal models. They struggle with accurately inferring the sufficient causes of an event, as it requires considering all possible counterfactual scenarios with each event removed or replaced except the outcome and the possible sufficient cause event.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"evolution_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"simple\",\n          \"reasoning\",\n          \"multi_context\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"episode_done\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "from phoenix.trace import using_project\n",
        "from ragas.testset.evolutions import multi_context, reasoning, simple\n",
        "from ragas.testset.generator import TestsetGenerator\n",
        "\n",
        "TEST_SIZE = 25\n",
        "\n",
        "# generator with openai models\n",
        "generator = TestsetGenerator.with_openai()\n",
        "\n",
        "# set question type distribution\n",
        "distribution = {simple: 0.5, reasoning: 0.25, multi_context: 0.25}\n",
        "\n",
        "# generate testset\n",
        "with using_project(\"ragas-testset\"):\n",
        "    testset = generator.generate_with_llamaindex_docs(\n",
        "        documents, test_size=TEST_SIZE, distributions=distribution\n",
        "    )\n",
        "test_df = testset.to_pandas()\n",
        "test_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bb9ffac",
      "metadata": {
        "id": "9bb9ffac"
      },
      "source": [
        "You are free to change the question type distribution according to your needs. Since we now have our test dataset ready, letâ€™s move on and build a simple RAG pipeline using LlamaIndex."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ded50764-cd14-402b-93fd-0e8377b88ddd",
      "metadata": {
        "id": "ded50764-cd14-402b-93fd-0e8377b88ddd"
      },
      "source": [
        "## 5. Build Your RAG Application With LlamaIndex"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff9c7460",
      "metadata": {
        "id": "ff9c7460"
      },
      "source": [
        "LlamaIndex is an easy to use and flexible framework for building RAG applications. For the sake of simplicity, we use the default LLM (gpt-3.5-turbo) and embedding models (openai-ada-2)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1eba224",
      "metadata": {
        "id": "e1eba224"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from phoenix.trace import using_project\n",
        "\n",
        "\n",
        "def build_query_engine(documents):\n",
        "    vector_index = VectorStoreIndex.from_documents(\n",
        "        documents,\n",
        "        embed_model=OpenAIEmbedding(),\n",
        "    )\n",
        "    query_engine = vector_index.as_query_engine(similarity_top_k=2)\n",
        "    return query_engine\n",
        "\n",
        "\n",
        "with using_project(\"indexing\"):\n",
        "    # By assigning a project name, the instrumentation will send all the embeddings to the indexing project\n",
        "    query_engine = build_query_engine(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b3a10b4",
      "metadata": {
        "id": "6b3a10b4"
      },
      "source": [
        "If you check Phoenix, you should see embedding spans from when your corpus data was indexed. Export and save those embeddings into a dataframe for visualization later in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5c6e3bc",
      "metadata": {
        "id": "c5c6e3bc",
        "outputId": "f11b88f7-2401-4fbd-dc8f-7dafd5160d80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 text  \\\n",
              "context.span_id  position                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
              "47f00c4c8c0616d9 0         page_label: 1\\nfile_path: /content/Papers/Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment.pdf\\n\\nTRUSTWORTHY LLM S:ASURVEY AND GUIDELINE FOR\\nEVALUATING LARGE LANGUAGE MODELS â€™ ALIGNMENT\\nYang Liuâˆ—Yuanshun Yaoâˆ—Jean-Francois Ton Xiaoying Zhang Ruocheng Guo\\nHao Cheng Yegor Klochkov Muhammad Faaiz Taufiq Hang Li\\nByteDance Research\\nAugust 9, 2023\\nABSTRACT\\nEnsuring alignment, which refers to making models behave in accordance with human intentions [ 1,2],\\nhas become a critical task before deploying large language models (LLMs) in real-world applications.\\nFor instance, OpenAI devoted six months to iteratively aligning GPT-4 before its release [ 3]. However,\\na major challenge faced by practitioners is the lack of clear guidance on evaluating whether LLM\\noutputs align with social norms, values, and regulations. This obstacle hinders systematic iteration\\nand deployment of LLMs. To address this issue, this paper presents a comprehensive survey of\\nkey dimensions that are crucial to consider when assessing LLM trustworthiness. The survey\\ncovers seven major categories of LLM trustworthiness: reliability, safety, fairness, resistance to\\nmisuse, explainability and reasoning, adherence to social norms, and robustness. Each major\\ncategory is further divided into several sub-categories, resulting in a total of 29 sub-categories.\\nAdditionally, a subset of 8 sub-categories is selected for further investigation, where corresponding\\nmeasurement studies are designed and conducted on several widely-used LLMs. The measurement\\nresults indicate that, in general, more aligned models tend to perform better in terms of overall\\ntrustworthiness. However, the effectiveness of alignment varies across the different trustworthiness\\ncategories considered. This highlights the importance of conducting more fine-grained analyses,\\ntesting, and making continuous improvements on LLM alignment. By shedding light on these key\\ndimensions of LLM trustworthiness, this paper aims to provide valuable insights and guidance to\\npractitioners in the field. Understanding and addressing these concerns will be crucial in achieving\\nreliable and ethically sound deployment of LLMs in various applications.\\nContent Warning : This document contains content that some may find disturbing or offen-\\nsive, including content that is discriminative, hateful, or violent in nature.\\nâˆ—YL and YY are listed alphabetically and co-led the work. Correspond to {yang.liu01, kevin.yao}@bytedance.com.arXiv:2308.05374v2  [cs.AI]  21 Mar 2024   \n",
              "                 1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               page_label: 2\\nfile_path: /content/Papers/Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment.pdf\\n\\nTrustworthy LLMs\\nContents\\n1 Introduction 4\\n2 Background 6\\n3 Taxonomy Overview 7\\n4 Reliability 9\\n4.1 Misinformation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n4.2 Hallucination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\\n4.3 Inconsistency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\\n4.4 Miscalibration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\\n4.5 Sycophancy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n5 Safety 13\\n5.1 Violence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\\n5.2 Unlawful Conduct . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\\n5.3 Harms to Minor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n5.4 Adult Content . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n5.5 Mental Health Issues . . . . . . . . . . . . . . .   \n",
              "                 2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            page_label: 2\\nfile_path: /content/Papers/Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment.pdf\\n\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n5.4 Adult Content . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n5.5 Mental Health Issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n5.6 Privacy Violation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n6 Fairness 16\\n6.1 Injustice . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n6.2 Stereotype Bias . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n6.3 Preference Bias . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\n6.4 Disparate Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\n7 Resistance to Misuse 18\\n7.1 Propagandistic Misuse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\n7.2 Cyberattack Misuse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\n7.3 Social-engineering Misuse . . . . . . . . . . .   \n",
              "                 3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     page_label: 2\\nfile_path: /content/Papers/Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment.pdf\\n\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\n7.2 Cyberattack Misuse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\n7.3 Social-engineering Misuse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\n7.4 Leaking Copyrighted Content . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\n8 Explainability and Reasoning 21\\n8.1 Lack of Interpretability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\n8.2 Limited General Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\\n8.3 Limited Causal Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\\n9 Social Norm 24\\n9.1 Toxicity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\\n9.2 Unawareness of Emotions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\\n2   \n",
              "                 4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   page_label: 3\\nfile_path: /content/Papers/Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment.pdf\\n\\nTrustworthy LLMs\\n9.3 Cultural Insensitivity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\\n10 Robustness 26\\n10.1 Prompt Attacks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\\n10.2 Paradigm and Distribution Shifts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\\n10.3 Interventional Effect . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\\n10.4 Poisoning Attacks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\\n11 Case Studies: Designs and Results 28\\n11.1 Overall Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\\n11.2 Hallucination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\\n11.3 Safety . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\\n11.4 Fairness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\\n11.5 Miscalibration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              vector  \n",
              "context.span_id  position                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
              "47f00c4c8c0616d9 0         [-0.011851692572236061, -0.002525044372305274, -0.0023376254830509424, -0.02423497475683689, 0.00485244719311595, 0.022694731131196022, -0.019791441038250923, 0.016492867842316628, -0.012356020510196686, -0.04435354471206665, 0.018005849793553352, 0.015429691411554813, -0.02084098756313324, 0.027301829308271408, -0.002703944221138954, 0.007694399915635586, 0.020227616652846336, -0.0033309459686279297, -0.002818099455907941, -0.006825457327067852, -0.026674827560782433, -0.012062964960932732, -0.0476248599588871, -0.01279901061207056, 0.0006397801334969699, 0.022708361968398094, 0.02790156938135624, -0.025638911873102188, -0.024548474699258804, -0.006273423321545124, -0.003847199957817793, 0.00319975265301764, -0.034648653119802475, 0.00961629580706358, -0.0027175748255103827, -0.012199269607663155, 0.013916708528995514, -0.003213383024558425, 0.028460418805480003, -0.010911190882325172, 0.010604504495859146, 0.023348992690443993, 0.013500979170203209, -0.017256174236536026, 0.0034025057684630156, 0.018251197412610054, 0.023076383396983147, -0.026156870648264885, -0.017310695722699165, 0.015129820443689823, 0.028106026351451874, 0.023730646818876266, -0.025679804384708405, -0.013385120779275894, 0.0038608303293585777, 0.00811012927442789, 0.014230209402740002, 0.024303127080202103, -0.007708030287176371, -0.014720906503498554, 0.00015770878235343844, -0.003884683595970273, -0.012778564356267452, 0.028187809512019157, 0.002373405499383807, -0.030423207208514214, -0.02724730782210827, 0.03876505419611931, 0.009037001058459282, -0.023348992690443993, 0.013078435324132442, 0.002013901714235544, 0.024330386891961098, 0.004814963322132826, 0.03045046702027321, -0.006218901369720697, -0.013828110881149769, -0.020977292209863663, -0.014952624216675758, 0.0023631826043128967, -0.0011688127415254712, -0.002586381509900093, 0.012437802739441395, 0.02108633518218994, 0.0009541328181512654, 0.009718524292111397, 0.009193751029670238, -0.02084098756313324, -0.01572956144809723, -0.0030821897089481354, 0.0024415578227490187, 0.004089140798896551, 0.012356020510196686, -0.00887343566864729, -0.018128523603081703, 0.010352341458201408, 0.004743403289467096, 0.013875817880034447, 0.009309610351920128, -0.028951115906238556, ...]  \n",
              "                 1                      [-0.0033576106652617455, 0.017739348113536835, -0.0003697115753311664, -0.03139442577958107, -0.003971952944993973, 0.018038861453533173, -0.027691353112459183, 0.006208091042935848, -0.0016286028549075127, -0.03980802372097969, 0.007127051707357168, 0.020543880760669708, -0.020829780027270317, 0.025308862328529358, 0.0013018612517043948, 0.006858170963823795, 0.0202579814940691, -0.01121812965720892, -0.0037371073849499226, -0.008699496276676655, -0.03003300167620182, 0.001846430590376258, -0.03931790962815285, -0.010054112412035465, 0.006579078733921051, 0.02812700904905796, 0.01877402886748314, -0.02799086645245552, -0.014294946566224098, 0.007344879675656557, -0.006371461786329746, -0.004812631756067276, -0.027963638305664062, 0.0008330210112035275, -0.0002503742871340364, -0.011987334117293358, 0.01986316777765751, 0.002816444728523493, 0.020448580384254456, -0.012184740044176579, 0.0020387317053973675, 0.01752151921391487, 0.0018089914228767157, -0.013770798221230507, -0.010959458537399769, 0.01712670736014843, 0.02175554633140564, -0.02801809459924698, -0.014431089162826538, 0.02146964892745018, 0.03912730887532234, 0.016269009560346603, -0.014540002681314945, -0.010823316872119904, 0.015465770848095417, 0.017834646627306938, 0.00597664900124073, 0.026683900505304337, 0.01414518989622593, -0.017099479213356972, 0.007903063669800758, -0.02053026668727398, -0.01959088444709778, 0.004764982033520937, -0.010605488903820515, -0.022218430414795876, -0.024355866014957428, 0.041006073355674744, 0.008794795721769333, -0.030278058722615242, 0.010428504087030888, 0.0016532785957679152, 0.010435311123728752, -0.023321183398365974, 0.016377924010157585, -0.012429796159267426, -0.012293653562664986, -0.03855551406741142, -0.007828185334801674, 0.001991081750020385, 0.011476799845695496, -0.00950954295694828, 0.006388479843735695, 0.013695919886231422, 0.016500452533364296, 0.004264659248292446, 0.01901908591389656, -0.0074537936598062515, -0.0211973637342453, -0.008093662559986115, 0.00696368096396327, 0.01496204361319542, 0.006892206147313118, 0.0004054489254485816, -0.021292662248015404, 0.02122459188103676, -0.0008589731296524405, 0.017031406983733177, 0.008148119784891605, -0.029134461656212807, ...]  \n",
              "                 2                    [-0.006365083158016205, 0.00787142850458622, 0.003343813121318817, -0.02854851260781288, 0.005109223071485758, 0.025885814800858498, -0.0128811439499259, 0.0066498820669949055, -0.0008955723023973405, -0.04364628344774246, 0.014123278670012951, 0.017760468646883965, -0.0162232406437397, 0.01887221448123455, 3.173963341396302e-05, 0.012737028300762177, 0.021356483921408653, -0.013821323402225971, -0.010691966861486435, 0.0023676049895584583, -0.02156236208975315, 0.015235023573040962, -0.03574054315686226, 0.00767927523702383, -0.011865475215017796, 0.027917150408029556, 0.022578030824661255, -0.02893281914293766, -0.01366348285228014, -0.0012567178346216679, -0.009806688874959946, -0.006080284249037504, -0.02788970060646534, 0.0003396998508833349, -0.010348835960030556, 0.0023058413062244654, 0.009683161042630672, -0.009072387591004372, 0.024787794798612595, -0.023346643894910812, 0.004189631436020136, 0.01310074795037508, -0.008365537971258163, -0.010040017776191235, -0.008818470872938633, 0.028466161340475082, 0.026530900970101357, -0.023923104628920555, -0.011021372862160206, 0.01544090174138546, 0.04139534384012222, 0.009655710309743881, -0.017142832279205322, -0.0061248913407325745, 0.00800181832164526, 0.00826259795576334, 0.020615320652723312, 0.029783785343170166, 0.007693000603467226, -0.007260655518621206, 0.011762536130845547, -0.008468477055430412, -0.016634998843073845, -0.006145479157567024, -0.022413326427340508, -0.023538798093795776, -0.024005455896258354, 0.053501009941101074, 0.011824299581348896, -0.021878043189644814, 0.020574145019054413, 0.009943940676748753, 0.01869378611445427, -0.023195665329694748, 0.021178055554628372, -0.019544750452041626, -0.01383504830300808, -0.034203313291072845, -0.0046322704292833805, 0.009916490875184536, 0.01383504830300808, -0.011748811230063438, -0.008235148154199123, 0.01427425630390644, 0.014933068305253983, 0.01381446048617363, 0.021932942792773247, 0.0018065855838358402, -0.02164471335709095, -0.006509197875857353, 0.0015989912208169699, 0.017033031210303307, 0.0060940091498196125, 0.009243953041732311, -0.025364255532622337, 0.015729133039712906, -0.005335689522325993, 0.015372276306152344, 0.018982015550136566, -0.02051924355328083, ...]  \n",
              "                 3             [-0.0034598209895193577, 0.006590298842638731, 0.010497820563614368, -0.029339008033275604, -0.0062781088054180145, 0.015465416945517063, -0.025716230273246765, 0.00861095730215311, -0.004219712223857641, -0.041936393827199936, 0.012604246847331524, 0.01903330348432064, -0.011677968315780163, 0.01991155371069908, 0.006168327294290066, 0.015465416945517063, 0.01705724373459816, -0.013921620324254036, -0.00044427052489481866, 0.00681329146027565, -0.015739869326353073, 0.017592426389455795, -0.02835097722709179, 0.007060299161821604, -0.003070441074669361, 0.029284117743372917, 0.022120898589491844, -0.031150396913290024, -0.008830520324409008, -0.0036742372903972864, -0.0028423022013157606, 0.00425744941458106, -0.03469083830714226, 0.01643972471356392, -0.0016269907355308533, -0.006549130659550428, 0.010401762090623379, -0.003200806211680174, 0.02191505953669548, -0.029311563819646835, -0.001596972462721169, 0.012027896009385586, -0.0019160239025950432, -0.011973004788160324, -0.014408773742616177, 0.01648089289665222, 0.027911853045225143, -0.02113286778330803, -0.009228476323187351, 0.014175488613545895, 0.03433404862880707, 0.017523813992738724, -0.01896469108760357, -0.003279711352661252, -0.00647365627810359, 0.008562928065657616, 0.012213150970637798, 0.025496669113636017, 0.008892271667718887, -0.004624530207365751, 0.0033877771347761154, -0.0065011014230549335, -0.015369358472526073, -0.004480442497879267, -0.017990384250879288, -0.026018129661679268, -0.027459006756544113, 0.05003275349736214, -0.004377522971481085, -0.02032323181629181, 0.022957980632781982, 0.015396804548799992, 0.013708919286727905, -0.02165432833135128, 0.013468773104250431, -0.015506585128605366, -0.014202934689819813, -0.03993288800120354, -0.014655781909823418, 0.005615991074591875, 0.01205534115433693, -0.011108478531241417, 0.013990233652293682, 0.011129062622785568, 0.018374618142843246, 0.0036742372903972864, 0.022875644266605377, 0.005176866427063942, -0.027019882574677467, -0.0014245817437767982, 0.007890518754720688, 0.01343446597456932, 0.013187458738684654, 0.011643661186099052, -0.027486450970172882, 0.022710971534252167, -0.003195660188794136, 0.01955476403236389, 0.031122950837016106, -0.031122950837016106, ...]  \n",
              "                 4                     [-0.013235564343631268, 0.014396337792277336, 0.004821674432605505, -0.014231494627892971, 0.010955228470265865, 0.022762149572372437, -0.017761893570423126, 0.008352073840796947, -0.002026202157139778, -0.0487387478351593, 0.019973546266555786, 0.016278302296996117, -0.023613840341567993, 0.010865938849747181, -0.0024966932833194733, 0.007589672692120075, 0.02225388213992119, -0.0019008524250239134, -0.006380819715559483, -0.0008564138552173972, -0.027528876438736916, 0.0019420633325353265, -0.038848135620355606, -0.007404223550111055, 0.009320530109107494, 0.017844315618276596, 0.020303232595324516, -0.02436937391757965, -0.02307809889316559, -0.005044899880886078, -0.005546299275010824, 0.0068204025737941265, -0.03211702033877373, 0.010089799761772156, -0.002129229484125972, -0.013105063699185848, 0.018201477825641632, -0.0004194066859781742, 0.02299567684531212, -0.022198934108018875, -0.0013616766082122922, 0.021635718643665314, 0.00112299679312855, -0.012191555462777615, -0.0027868866454809904, 0.02166319265961647, 0.028270671144127846, -0.01928669773042202, -0.01501450128853321, 0.014135335572063923, 0.04277690500020981, 0.019163064658641815, -0.016443146392703056, -0.007624015212059021, 0.01590740494430065, 0.015605190768837929, 0.016717884689569473, 0.03489188849925995, 0.00849631242454052, -0.00878478866070509, 0.006030527409166098, -0.008036123588681221, -0.02436937391757965, 0.012445689179003239, 0.002975769806653261, -0.03439735621213913, -0.02322920598089695, 0.03420504182577133, 0.02527601458132267, -0.013558383099734783, 0.007981176488101482, 0.013338591903448105, 0.016429409384727478, -0.019149327650666237, 0.031869757920503616, -0.015330451540648937, -0.01230832003057003, -0.03802391514182091, -0.005189137998968363, 0.006837573833763599, 0.010570594109594822, -0.006373951211571693, -0.00956092681735754, 0.017706947401165962, 0.0067414152435958385, 0.0046327910386025906, 0.01667667366564274, -0.016841517761349678, -0.017363522201776505, -0.001255215029232204, 0.013056984171271324, 0.01424523163586855, 0.0026254772674292326, -0.006600611377507448, -0.024259477853775024, 0.016209617257118225, -0.0020794328302145004, 0.0168277807533741, 0.015893667936325073, -0.01831137388944626, ...]  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-586d0624-08dc-489c-9ef6-2830bc00bc0e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>vector</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>context.span_id</th>\n",
              "      <th>position</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"5\" valign=\"top\">47f00c4c8c0616d9</th>\n",
              "      <th>0</th>\n",
              "      <td>page_label: 1\\nfile_path: /content/Papers/Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment.pdf\\n\\nTRUSTWORTHY LLM S:ASURVEY AND GUIDELINE FOR\\nEVALUATING LARGE LANGUAGE MODELS â€™ ALIGNMENT\\nYang Liuâˆ—Yuanshun Yaoâˆ—Jean-Francois Ton Xiaoying Zhang Ruocheng Guo\\nHao Cheng Yegor Klochkov Muhammad Faaiz Taufiq Hang Li\\nByteDance Research\\nAugust 9, 2023\\nABSTRACT\\nEnsuring alignment, which refers to making models behave in accordance with human intentions [ 1,2],\\nhas become a critical task before deploying large language models (LLMs) in real-world applications.\\nFor instance, OpenAI devoted six months to iteratively aligning GPT-4 before its release [ 3]. However,\\na major challenge faced by practitioners is the lack of clear guidance on evaluating whether LLM\\noutputs align with social norms, values, and regulations. This obstacle hinders systematic iteration\\nand deployment of LLMs. To address this issue, this paper presents a comprehensive survey of\\nkey dimensions that are crucial to consider when assessing LLM trustworthiness. The survey\\ncovers seven major categories of LLM trustworthiness: reliability, safety, fairness, resistance to\\nmisuse, explainability and reasoning, adherence to social norms, and robustness. Each major\\ncategory is further divided into several sub-categories, resulting in a total of 29 sub-categories.\\nAdditionally, a subset of 8 sub-categories is selected for further investigation, where corresponding\\nmeasurement studies are designed and conducted on several widely-used LLMs. The measurement\\nresults indicate that, in general, more aligned models tend to perform better in terms of overall\\ntrustworthiness. However, the effectiveness of alignment varies across the different trustworthiness\\ncategories considered. This highlights the importance of conducting more fine-grained analyses,\\ntesting, and making continuous improvements on LLM alignment. By shedding light on these key\\ndimensions of LLM trustworthiness, this paper aims to provide valuable insights and guidance to\\npractitioners in the field. Understanding and addressing these concerns will be crucial in achieving\\nreliable and ethically sound deployment of LLMs in various applications.\\nContent Warning : This document contains content that some may find disturbing or offen-\\nsive, including content that is discriminative, hateful, or violent in nature.\\nâˆ—YL and YY are listed alphabetically and co-led the work. Correspond to {yang.liu01, kevin.yao}@bytedance.com.arXiv:2308.05374v2  [cs.AI]  21 Mar 2024</td>\n",
              "      <td>[-0.011851692572236061, -0.002525044372305274, -0.0023376254830509424, -0.02423497475683689, 0.00485244719311595, 0.022694731131196022, -0.019791441038250923, 0.016492867842316628, -0.012356020510196686, -0.04435354471206665, 0.018005849793553352, 0.015429691411554813, -0.02084098756313324, 0.027301829308271408, -0.002703944221138954, 0.007694399915635586, 0.020227616652846336, -0.0033309459686279297, -0.002818099455907941, -0.006825457327067852, -0.026674827560782433, -0.012062964960932732, -0.0476248599588871, -0.01279901061207056, 0.0006397801334969699, 0.022708361968398094, 0.02790156938135624, -0.025638911873102188, -0.024548474699258804, -0.006273423321545124, -0.003847199957817793, 0.00319975265301764, -0.034648653119802475, 0.00961629580706358, -0.0027175748255103827, -0.012199269607663155, 0.013916708528995514, -0.003213383024558425, 0.028460418805480003, -0.010911190882325172, 0.010604504495859146, 0.023348992690443993, 0.013500979170203209, -0.017256174236536026, 0.0034025057684630156, 0.018251197412610054, 0.023076383396983147, -0.026156870648264885, -0.017310695722699165, 0.015129820443689823, 0.028106026351451874, 0.023730646818876266, -0.025679804384708405, -0.013385120779275894, 0.0038608303293585777, 0.00811012927442789, 0.014230209402740002, 0.024303127080202103, -0.007708030287176371, -0.014720906503498554, 0.00015770878235343844, -0.003884683595970273, -0.012778564356267452, 0.028187809512019157, 0.002373405499383807, -0.030423207208514214, -0.02724730782210827, 0.03876505419611931, 0.009037001058459282, -0.023348992690443993, 0.013078435324132442, 0.002013901714235544, 0.024330386891961098, 0.004814963322132826, 0.03045046702027321, -0.006218901369720697, -0.013828110881149769, -0.020977292209863663, -0.014952624216675758, 0.0023631826043128967, -0.0011688127415254712, -0.002586381509900093, 0.012437802739441395, 0.02108633518218994, 0.0009541328181512654, 0.009718524292111397, 0.009193751029670238, -0.02084098756313324, -0.01572956144809723, -0.0030821897089481354, 0.0024415578227490187, 0.004089140798896551, 0.012356020510196686, -0.00887343566864729, -0.018128523603081703, 0.010352341458201408, 0.004743403289467096, 0.013875817880034447, 0.009309610351920128, -0.028951115906238556, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>page_label: 2\\nfile_path: /content/Papers/Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment.pdf\\n\\nTrustworthy LLMs\\nContents\\n1 Introduction 4\\n2 Background 6\\n3 Taxonomy Overview 7\\n4 Reliability 9\\n4.1 Misinformation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n4.2 Hallucination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\\n4.3 Inconsistency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\\n4.4 Miscalibration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\\n4.5 Sycophancy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n5 Safety 13\\n5.1 Violence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\\n5.2 Unlawful Conduct . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\\n5.3 Harms to Minor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n5.4 Adult Content . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n5.5 Mental Health Issues . . . . . . . . . . . . . . .</td>\n",
              "      <td>[-0.0033576106652617455, 0.017739348113536835, -0.0003697115753311664, -0.03139442577958107, -0.003971952944993973, 0.018038861453533173, -0.027691353112459183, 0.006208091042935848, -0.0016286028549075127, -0.03980802372097969, 0.007127051707357168, 0.020543880760669708, -0.020829780027270317, 0.025308862328529358, 0.0013018612517043948, 0.006858170963823795, 0.0202579814940691, -0.01121812965720892, -0.0037371073849499226, -0.008699496276676655, -0.03003300167620182, 0.001846430590376258, -0.03931790962815285, -0.010054112412035465, 0.006579078733921051, 0.02812700904905796, 0.01877402886748314, -0.02799086645245552, -0.014294946566224098, 0.007344879675656557, -0.006371461786329746, -0.004812631756067276, -0.027963638305664062, 0.0008330210112035275, -0.0002503742871340364, -0.011987334117293358, 0.01986316777765751, 0.002816444728523493, 0.020448580384254456, -0.012184740044176579, 0.0020387317053973675, 0.01752151921391487, 0.0018089914228767157, -0.013770798221230507, -0.010959458537399769, 0.01712670736014843, 0.02175554633140564, -0.02801809459924698, -0.014431089162826538, 0.02146964892745018, 0.03912730887532234, 0.016269009560346603, -0.014540002681314945, -0.010823316872119904, 0.015465770848095417, 0.017834646627306938, 0.00597664900124073, 0.026683900505304337, 0.01414518989622593, -0.017099479213356972, 0.007903063669800758, -0.02053026668727398, -0.01959088444709778, 0.004764982033520937, -0.010605488903820515, -0.022218430414795876, -0.024355866014957428, 0.041006073355674744, 0.008794795721769333, -0.030278058722615242, 0.010428504087030888, 0.0016532785957679152, 0.010435311123728752, -0.023321183398365974, 0.016377924010157585, -0.012429796159267426, -0.012293653562664986, -0.03855551406741142, -0.007828185334801674, 0.001991081750020385, 0.011476799845695496, -0.00950954295694828, 0.006388479843735695, 0.013695919886231422, 0.016500452533364296, 0.004264659248292446, 0.01901908591389656, -0.0074537936598062515, -0.0211973637342453, -0.008093662559986115, 0.00696368096396327, 0.01496204361319542, 0.006892206147313118, 0.0004054489254485816, -0.021292662248015404, 0.02122459188103676, -0.0008589731296524405, 0.017031406983733177, 0.008148119784891605, -0.029134461656212807, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>page_label: 2\\nfile_path: /content/Papers/Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment.pdf\\n\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n5.4 Adult Content . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n5.5 Mental Health Issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n5.6 Privacy Violation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n6 Fairness 16\\n6.1 Injustice . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n6.2 Stereotype Bias . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n6.3 Preference Bias . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\n6.4 Disparate Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\n7 Resistance to Misuse 18\\n7.1 Propagandistic Misuse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\n7.2 Cyberattack Misuse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\n7.3 Social-engineering Misuse . . . . . . . . . . .</td>\n",
              "      <td>[-0.006365083158016205, 0.00787142850458622, 0.003343813121318817, -0.02854851260781288, 0.005109223071485758, 0.025885814800858498, -0.0128811439499259, 0.0066498820669949055, -0.0008955723023973405, -0.04364628344774246, 0.014123278670012951, 0.017760468646883965, -0.0162232406437397, 0.01887221448123455, 3.173963341396302e-05, 0.012737028300762177, 0.021356483921408653, -0.013821323402225971, -0.010691966861486435, 0.0023676049895584583, -0.02156236208975315, 0.015235023573040962, -0.03574054315686226, 0.00767927523702383, -0.011865475215017796, 0.027917150408029556, 0.022578030824661255, -0.02893281914293766, -0.01366348285228014, -0.0012567178346216679, -0.009806688874959946, -0.006080284249037504, -0.02788970060646534, 0.0003396998508833349, -0.010348835960030556, 0.0023058413062244654, 0.009683161042630672, -0.009072387591004372, 0.024787794798612595, -0.023346643894910812, 0.004189631436020136, 0.01310074795037508, -0.008365537971258163, -0.010040017776191235, -0.008818470872938633, 0.028466161340475082, 0.026530900970101357, -0.023923104628920555, -0.011021372862160206, 0.01544090174138546, 0.04139534384012222, 0.009655710309743881, -0.017142832279205322, -0.0061248913407325745, 0.00800181832164526, 0.00826259795576334, 0.020615320652723312, 0.029783785343170166, 0.007693000603467226, -0.007260655518621206, 0.011762536130845547, -0.008468477055430412, -0.016634998843073845, -0.006145479157567024, -0.022413326427340508, -0.023538798093795776, -0.024005455896258354, 0.053501009941101074, 0.011824299581348896, -0.021878043189644814, 0.020574145019054413, 0.009943940676748753, 0.01869378611445427, -0.023195665329694748, 0.021178055554628372, -0.019544750452041626, -0.01383504830300808, -0.034203313291072845, -0.0046322704292833805, 0.009916490875184536, 0.01383504830300808, -0.011748811230063438, -0.008235148154199123, 0.01427425630390644, 0.014933068305253983, 0.01381446048617363, 0.021932942792773247, 0.0018065855838358402, -0.02164471335709095, -0.006509197875857353, 0.0015989912208169699, 0.017033031210303307, 0.0060940091498196125, 0.009243953041732311, -0.025364255532622337, 0.015729133039712906, -0.005335689522325993, 0.015372276306152344, 0.018982015550136566, -0.02051924355328083, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>page_label: 2\\nfile_path: /content/Papers/Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment.pdf\\n\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\n7.2 Cyberattack Misuse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\n7.3 Social-engineering Misuse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\n7.4 Leaking Copyrighted Content . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\n8 Explainability and Reasoning 21\\n8.1 Lack of Interpretability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\n8.2 Limited General Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\\n8.3 Limited Causal Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\\n9 Social Norm 24\\n9.1 Toxicity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\\n9.2 Unawareness of Emotions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\\n2</td>\n",
              "      <td>[-0.0034598209895193577, 0.006590298842638731, 0.010497820563614368, -0.029339008033275604, -0.0062781088054180145, 0.015465416945517063, -0.025716230273246765, 0.00861095730215311, -0.004219712223857641, -0.041936393827199936, 0.012604246847331524, 0.01903330348432064, -0.011677968315780163, 0.01991155371069908, 0.006168327294290066, 0.015465416945517063, 0.01705724373459816, -0.013921620324254036, -0.00044427052489481866, 0.00681329146027565, -0.015739869326353073, 0.017592426389455795, -0.02835097722709179, 0.007060299161821604, -0.003070441074669361, 0.029284117743372917, 0.022120898589491844, -0.031150396913290024, -0.008830520324409008, -0.0036742372903972864, -0.0028423022013157606, 0.00425744941458106, -0.03469083830714226, 0.01643972471356392, -0.0016269907355308533, -0.006549130659550428, 0.010401762090623379, -0.003200806211680174, 0.02191505953669548, -0.029311563819646835, -0.001596972462721169, 0.012027896009385586, -0.0019160239025950432, -0.011973004788160324, -0.014408773742616177, 0.01648089289665222, 0.027911853045225143, -0.02113286778330803, -0.009228476323187351, 0.014175488613545895, 0.03433404862880707, 0.017523813992738724, -0.01896469108760357, -0.003279711352661252, -0.00647365627810359, 0.008562928065657616, 0.012213150970637798, 0.025496669113636017, 0.008892271667718887, -0.004624530207365751, 0.0033877771347761154, -0.0065011014230549335, -0.015369358472526073, -0.004480442497879267, -0.017990384250879288, -0.026018129661679268, -0.027459006756544113, 0.05003275349736214, -0.004377522971481085, -0.02032323181629181, 0.022957980632781982, 0.015396804548799992, 0.013708919286727905, -0.02165432833135128, 0.013468773104250431, -0.015506585128605366, -0.014202934689819813, -0.03993288800120354, -0.014655781909823418, 0.005615991074591875, 0.01205534115433693, -0.011108478531241417, 0.013990233652293682, 0.011129062622785568, 0.018374618142843246, 0.0036742372903972864, 0.022875644266605377, 0.005176866427063942, -0.027019882574677467, -0.0014245817437767982, 0.007890518754720688, 0.01343446597456932, 0.013187458738684654, 0.011643661186099052, -0.027486450970172882, 0.022710971534252167, -0.003195660188794136, 0.01955476403236389, 0.031122950837016106, -0.031122950837016106, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>page_label: 3\\nfile_path: /content/Papers/Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment.pdf\\n\\nTrustworthy LLMs\\n9.3 Cultural Insensitivity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\\n10 Robustness 26\\n10.1 Prompt Attacks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\\n10.2 Paradigm and Distribution Shifts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\\n10.3 Interventional Effect . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\\n10.4 Poisoning Attacks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\\n11 Case Studies: Designs and Results 28\\n11.1 Overall Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\\n11.2 Hallucination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\\n11.3 Safety . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\\n11.4 Fairness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\\n11.5 Miscalibration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</td>\n",
              "      <td>[-0.013235564343631268, 0.014396337792277336, 0.004821674432605505, -0.014231494627892971, 0.010955228470265865, 0.022762149572372437, -0.017761893570423126, 0.008352073840796947, -0.002026202157139778, -0.0487387478351593, 0.019973546266555786, 0.016278302296996117, -0.023613840341567993, 0.010865938849747181, -0.0024966932833194733, 0.007589672692120075, 0.02225388213992119, -0.0019008524250239134, -0.006380819715559483, -0.0008564138552173972, -0.027528876438736916, 0.0019420633325353265, -0.038848135620355606, -0.007404223550111055, 0.009320530109107494, 0.017844315618276596, 0.020303232595324516, -0.02436937391757965, -0.02307809889316559, -0.005044899880886078, -0.005546299275010824, 0.0068204025737941265, -0.03211702033877373, 0.010089799761772156, -0.002129229484125972, -0.013105063699185848, 0.018201477825641632, -0.0004194066859781742, 0.02299567684531212, -0.022198934108018875, -0.0013616766082122922, 0.021635718643665314, 0.00112299679312855, -0.012191555462777615, -0.0027868866454809904, 0.02166319265961647, 0.028270671144127846, -0.01928669773042202, -0.01501450128853321, 0.014135335572063923, 0.04277690500020981, 0.019163064658641815, -0.016443146392703056, -0.007624015212059021, 0.01590740494430065, 0.015605190768837929, 0.016717884689569473, 0.03489188849925995, 0.00849631242454052, -0.00878478866070509, 0.006030527409166098, -0.008036123588681221, -0.02436937391757965, 0.012445689179003239, 0.002975769806653261, -0.03439735621213913, -0.02322920598089695, 0.03420504182577133, 0.02527601458132267, -0.013558383099734783, 0.007981176488101482, 0.013338591903448105, 0.016429409384727478, -0.019149327650666237, 0.031869757920503616, -0.015330451540648937, -0.01230832003057003, -0.03802391514182091, -0.005189137998968363, 0.006837573833763599, 0.010570594109594822, -0.006373951211571693, -0.00956092681735754, 0.017706947401165962, 0.0067414152435958385, 0.0046327910386025906, 0.01667667366564274, -0.016841517761349678, -0.017363522201776505, -0.001255215029232204, 0.013056984171271324, 0.01424523163586855, 0.0026254772674292326, -0.006600611377507448, -0.024259477853775024, 0.016209617257118225, -0.0020794328302145004, 0.0168277807533741, 0.015893667936325073, -0.01831137388944626, ...]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-586d0624-08dc-489c-9ef6-2830bc00bc0e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-586d0624-08dc-489c-9ef6-2830bc00bc0e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-586d0624-08dc-489c-9ef6-2830bc00bc0e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f921949f-4246-473e-9764-664bb518492d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f921949f-4246-473e-9764-664bb518492d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f921949f-4246-473e-9764-664bb518492d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "corpus_df",
              "summary": "{\n  \"name\": \"corpus_df\",\n  \"rows\": 125,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 125,\n        \"samples\": [\n          \"page_label: 11\\nfile_path: /content/Papers/Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment.pdf\\n\\nTrustworthy LLMs\\nAnswer the question based on the provided knowledge.\\nKnowledge : House of Anubis is a mystery television series developed for Nickelodeon based on the Dutch-\\nBelgian television series \\u2018Het Huis Anubis\\u2019. It first aired in September 2006 and the last episode was broad-\\ncast on December 4, 2009.\\nQuestion : The Dutch-Belgian television series that \\u2018House of Anubis\\u2019 was based on first aired in what year?\\n[ChatGPT]: The Dutch-Belgian television series that \\u2019House of Anubis\\u2019\\nwas based on first aired in an unspecified year.\\nFigure 6: Examples of intrinsic hallucination of using ChatGPT: While the queried series was first aired in 2006, as indicated in the\\nprovided knowledge. Access: June 2023.\\nWhich year was Jean-Francois Bryant Curry enter the NBA?\\n[text-davinci-003]: Jean-Francois Bryant Curry entered the NBA in 2019.\\nFigure 7: Examples of extrinsic hallucination of using text-davinci-003 : Jean-Francois Bryant Curry is a fabricated person and does\\nnot actually exist. Access: June 2023.\\nlimited to the randomness introduced in sampling the next tokens, errors in encoding [ 67,68] and decoding [ 69], the\\ntraining bias from imbalanced distributions, and over-reliance on memorized information [70] etc.\\nEvaluating and detecting hallucination is still an ongoing area [ 71]. The common evaluation task is text summarization,\\nand a simple metric would be the standard text similarity between LLM outputs and the reference texts, e.g.ROUGE [ 72]\\nand BLEU [ 73]. Another popular task is QA (question and answering) [ 74] where LLMs answer questions and we\\ncompute the text similarity between LLM answers and the ground-truth answers. A different evaluation approach is to\\ntrain truthfulness classifiers to label LLM outputs [ 75,76]. Last but not least, human evaluation is still one of the most\\ncommonly used approaches [77, 78, 69, 79].\\nMitigating hallucinations is an open problem. Currently, only a limited number of methods are proposed. One aspect\\nis to increase training data quality, e.g.building more faithful datasets [ 76,80] and data cleaning [ 81,82]. The other\\naspect is using different rewards in RLHF. For example, in dialogue, [ 83] a consistency reward which is the difference\\nbetween the generated template and the slot-value pairs extracted from inputs. In text summarization, [ 84] design the\\nreward by combining ROUGE and the multiple-choice cloze score to reduce hallucinations in summarized text. In\\naddition, leveraging an external knowledge base can also help [ 85,86,87,88]. Overall, we do not currently have a\\ngood mitigation strategy.\\n4.3 Inconsistency\\nLLMs have been reported to give inconsistent outputs [ 89,6,90,91]. It is shown that the models could fail to provide\\nthe same and consistent answers to different users, to the same user but in different sessions, and even in chats within\\nthe sessions of the same conversation. These inconsistent answers can create confusion among users and reduce user\\ntrust. The exact cause of inconsistency is unclear. But the randomness certainly plays a role, including randomness\\nin sampling tokens, model updates, hidden operations within the platform, or hardware specs. It is a signal that the\\nLLM might still lag behind in its reasoning capacities, another important consideration we will discuss in more detail in\\nSection 8.24\\nFor example, in Figure 8 we observe that LLMs behave inconsistently when prompting questions are asked in different\\nways. When asked to answer a simple algebra question, it failed to provide a correct answer; while asked to perform the\\ncalculation with steps, the ChatGPT was able to obtain the correct one. This requires users to be careful at prompting,\\ntherefore raising the bar of using LLMs to merely get correct answers, which ideally should not be the case, and of\\ncourse, reducing the trustworthiness of all the answers.\\nIn addition, it is also reported that LLMs can generate inconsistent responses for the same questions (but in different\\nsessions) [ 92]. This issue is related to the model\\u2019s power in logic reasoning (discussed in Section 8.2) but the cause\\nfor inconsistent responses can be more complicated.\",\n          \"page_label: 26\\nfile_path: /content/Papers/Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment.pdf\\n\\nTrustworthy LLMs\\nit is important to build high-quality locally collected datasets that reflect views from local users to align a model\\u2019s value\\nsystem. The literature has reported different levels of biases in LLMs towards different cultures and values. [ 272]\\nevaluates GPT-3\\u2019s cross-culture alignments and identified that the model performs significantly better when prompted\\nwith American context as compared to other cultures. [ 416] discussed the value conflicts of GPT-3 and argues for a\\nsolution that is better contextualized of societal harm and benefit. [ 417] performed an empirical analysis of GPT-3\\u2019s\\n\\u201cpersonality\\\", its value system, and its demographics.\\nIn response to the challenges, recent works have looked into the challenge of improving the LLMs\\u2019 sensitivity to cultural\\nvalues. For example, as discussed in Section 2, [ 1] proposed the idea of aligning LLMs using instructions with human\\nfeedback. [ 418] proposes a framework that AI can self-improve their harmless based on a list of user-specified rules or\\nprinciples. [ 42] discussed the possibility of training socially aligned models via simulating a society. [ 259] proposes a\\nmethod that aligns language models with cultural values via a target value dataset.\\n10 Robustness\\nWhile it is important to validate the performance of an LLM before it is released, it is equally important to test its\\nrobustness when deploying. There are multiple reasons why the LLM might not perform as desired when deployed.\\nThe errors in a prompt can cause the model\\u2019s failure in answering the question correctly. Malicious entities can attack\\nthe system by poking the LLM using maliciously altered prompts. The usefulness of a set of particular answers might\\nchange over time ( e.g.which state collects the highest state income tax). Finally, LLMs are trained on the massive data\\ncollected from the Internet where anyone, including attackers, can post content, and therefore influence LLMs\\u2019 training\\ndata, opening up the vulnerability of LLMs to poisoning attacks.\\n10.1 Prompt Attacks\\nLLMs are sensitive to the engineering of prompts. Previously, we have shown examples ( e.g.Figure 8) of LLMs being\\ninconsistent with their answers when prompted differently. But a more severe problem is that even non-detrimental\\ndeviations from perfectly structured prompts, such as grammatical and typo errors, can cause the LLMs to output wrong\\nand low-quality content. In this example shown in Figure 25, we ask the GPT-3 models the same question of whether a\\nstatement is reasonable or not (in the context of the claim can be justified ethically or morally). The model tends to\\nprovide different answers when prompted with one that contains typos.\\nIs the following sentence reasonable?\\nAs a parent, I usually give my kids lunch to take to school, but I didn\\u2019t\\ntoday because he behaved terribly today.\\n[GPT-3]: Yes, this sentence is reasonable.\\nIs the following sentence reasonable?\\nAs a pearent, I usaully give my kids lunchh to takee to scchool, but I\\ndidnt\\u2019t todayy becuase he behavaed terriblly todayu.\\n[GPT-3]: No, this sentence is not reasonable.\\nFigure 25: An example of GPT-3 showing a different answer when prompted the same question but with typos. Access: June 2023.\\nIn addition, LLMs can be vulnerable to prompt attacks [ 419,420,421]. For example, [ 420] shows that carefully\\ncontrolled adversarial perturbation can flip a GPT model\\u2019s answer when used to classify text inputs. Furthermore, we\\nfind that by twisting the prompting question in a certain way, one can solicit dangerous information that the model\\nchose to not answer. We have seen such an example in Figure 12.\\nOn the other hand, adversarial prompt engineering, if used properly, can serve the purpose of red teaming and provide\\nthe LLMs with a high-quality set of examples for alignment [ 422]. This observation resembles similarities to the\\nliterature on using adversarial examples to augment the training data [423].\\n10.2 Paradigm and Distribution Shifts\\nKnowledge bases that LLMs are trained on continue to shift [ 424,425]. For example, Figure 26 shows questions such\\nas \\u201cwho scored the most points in NBA history\\\" or \\u201cwho is the richest person in the world\\\" might have answers that\\nneed to be updated over time, or even in real-time.\\n26\",\n          \"page_label: 23\\nfile_path: /content/Papers/Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment.pdf\\n\\nTrustworthy LLMs\\nconstruct the best possible explanation or hypothesis from the available information. It is shown that GPT-3 can barely\\noutperform random guesses while GPT-4 can only solve 38% of the detective puzzles.\\nThe results cited above across different tasks underscore a continued gap between LLMs and human-like logical\\nreasoning ability. Moreover, a highly relevant challenge from the above studies is identifying answers from LLMs that\\ndo not reason logically, necessitating further research in the domain.\\nRecently, there exists a series of work that aims to improve LLMs in terms of their reasoning ability. As mentioned\\nin [388], these methods can be categorized into four types: prompt engineering, pretraining and continual training,\\nsupervised fine-tuning, and reinforcement learning. Below we discuss some of the relevant works from these categories.\\nAs mentioned before, prompt engineering techniques such as CoT, instruction tuning, and in-context learning can\\nenhance LLMs\\u2019 reasoning abilities. For example, Zhou et al. [ 389] propose Least-to-most prompting that results in\\nimproved reasoning capabilities. Least-to-most prompting asks LLMs to decompose each question into subquestions\\nand queries LLMs for answers to each subquestion. In [ 390,391], results show that continuing to train pretrained\\nLLMs on the same objective function using high-quality data from specific domains (e.g., Arxiv papers and code\\ndata) can improve their performance on down-stream tasks for these domains. In contrast, [ 392,393] show the\\neffectiveness of pretraining an LLM from scratch with data curated for tasks that require complex reasoning abilities.\\nSupervised fine-tuning is different from continuing to train as it trains LLMs for accurate predictions in downstream\\ntasks instead of continuing to train on language modeling objectives. Chung et al. [ 30] propose to add data augmented\\nby human-annotated CoT in multi-task fine-tuning. Fu et al. [ 394] show that LLMs\\u2019 improvement of reasoning ability\\ncan be distilled to smaller models by model specialization , which utilizes specialization data partially generated by\\nlarger models ( e.g.code-davinci-0025) to fine-tune smaller models. The specialization data includes multiple data\\nformats specifically designed for complex reasoning ( e.g.in-context CoT: combining CoT with questions and answers).\\nLi et al. [ 395] fine-tune LLMs on coding test data and introduce a filtering mechanism that checks whether the sampled\\nanswer can pass the example provided in the coding question. A series of work [ 396,397] leverages reinforcement\\nlearning to improve LLMs\\u2019 reasoning capabilities by designing novel reward models that can capture the crucial patterns\\n(e.g., rewards for intermediate reasoning steps in math problems) of specific reasoning problems such as math and\\ncoding. As reasoning can cover an extremely broad range of tasks, the evaluation of LLMs\\u2019 complex reasoning abilities\\nis challenging and requires benchmarking on a comprehensive set of tasks. Therefore, the Chain-of-thought hub [ 398]\\nis proposed to cover a wide range of complex reasoning tasks including math, science, symbol, and knowledge. It\\nspecifically focuses on the reasoning ability of LLMs following the few-shot chain-of-thought prompting [ 29] paradigm.\\nNext, we examine causal reasoning, which focuses on tasks requiring an understanding of specific aspects of causality.\\n8.3 Limited Causal Reasoning\\nUnlike logical reasoning, which derives conclusions based on premises, causal reasoning makes inferences about the\\nrelationships between events or states of the world, mostly by identifying cause-effect relationships. Causal reasoning\\ntasks specifically examine various aspects regarding LLMs\\u2019 understanding of causality, including inferring causal\\nrelationships among random variables ( e.g.temperature and latitude) [ 399] and events ( e.g.a person bumped against\\na table and a beer fell to the group) [ 358], answering counterfactual questions, and understanding rules of structural\\ncausal models [400] ( e.g.d-separation).\\nIn the task of inferring the necessary and sufficient cause of an event in a given chunk of text, Kiciman et al. [ 358] find\\nthat although GPT-4 can be quite accurate in making inferences of necessary cause, the accuracy for sufficient cause\\ninference is much lower. They conjecture that this is because inferring the sufficient causes of an event requires the\\nLLM to answer a large set of counterfactual questions.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"vector\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "from phoenix.trace.dsl.helpers import SpanQuery\n",
        "\n",
        "client = px.Client()\n",
        "corpus_df = px.Client().query_spans(\n",
        "    SpanQuery().explode(\n",
        "        \"embedding.embeddings\",\n",
        "        text=\"embedding.text\",\n",
        "        vector=\"embedding.vector\",\n",
        "    ),\n",
        "    project_name=\"indexing\",\n",
        ")\n",
        "corpus_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59e745b4",
      "metadata": {
        "id": "59e745b4"
      },
      "source": [
        "## 6. Evaluate Your LLM Application"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df6acfc5",
      "metadata": {
        "id": "df6acfc5"
      },
      "source": [
        "Ragas provides a comprehensive list of metrics that can be used to evaluate RAG pipelines both component-wise and end-to-end.\n",
        "\n",
        "To use Ragas, we first form an evaluation dataset comprised of a question, generated answer, retrieved context, and ground-truth answer (the actual expected answer for the given question)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2597314-d6de-412d-b00c-3e00297746e2",
      "metadata": {
        "id": "e2597314-d6de-412d-b00c-3e00297746e2",
        "outputId": "28970ed6-4e23-428f-bc27-7bbf6aecd96a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "30e0edd0a05c4afc8ac50bcd19330410",
            "4398727f5388495a8718ab09c6c48955",
            "4b4d447c9b81447e8d2aa447a80c8c16",
            "00d7e60b2fe9437f81d0297f89230d60",
            "6a3d11bcf73c4fd5a04cce38538a0b07",
            "08a04f1a14d9413ebb31ddc80868c44e",
            "cca53df0cdb946dba3fdc8e6e6ae43ce",
            "4e1c02e8fc344b19bebe527935a8c6e8",
            "f66180150e884a1e99c7cc4fa6141636",
            "eb569b6b4d3940d7a0247bfb980ac55f",
            "571f8ebc5f014469be4b12a87c7267f5"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "30e0edd0a05c4afc8ac50bcd19330410"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                                                                    question  \\\n",
              "0                             What challenges do LLMs face in understanding causality and performing causal reasoning tasks?   \n",
              "1                                                 How can the evaluation data be converted into training data for alignment?   \n",
              "2                                                                                   How robust are LLMs to typos in prompts?   \n",
              "3  Why is it important for Language Model Models (LLMs) to express uncertainty and abstain from answering certain questions?   \n",
              "4                                                  What are some challenges and alternatives in the LLM alignment algorithm?   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       answer  \\\n",
              "0                                                                                                                                                                                                                                                                                      LLMs face challenges in understanding causality and performing causal reasoning tasks, particularly in inferring the sufficient causes of events. This difficulty arises because inferring the sufficient causes requires considering a large set of counterfactual questions. While LLMs can be accurate in making inferences of necessary cause, their accuracy in inferring sufficient cause is notably lower due to the complexity of evaluating various counterfactual scenarios.   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                         The evaluation data can be converted into training data for alignment by leveraging the existing high-quality LLMs to judge if a model passes a certain test or not. This process can accelerate the evaluation task from manual work to a more automated approach, reducing the reliance on human labelers and speeding up the evaluation process.   \n",
              "2                                                                                                                                                                                                                                                                                            LLMs show varying levels of robustness to typos in prompts. The evaluation results indicate that none of the LLMs are extremely robust to typo attacks. Among the well-aligned LLMs, davinci has the smallest drop in consistency, while flan-t5-xxl exhibits the least amount of consistency downgrade. On the other hand, ChatGPT and GPT-4 demonstrate vulnerability against typo attacks, often providing inconsistent answers or marking the typoed prompts as nonsensical.   \n",
              "3                                                                                                                                                                                                                   Expressing uncertainty and abstaining from answering certain questions is important for Language Models (LLMs) to maintain trustworthiness and reliability. This approach helps prevent the generation of incorrect or inconsistent responses, which can reduce the overall trust in the answers provided by the LLMs. By acknowledging uncertainty and refraining from answering questions outside their scope or where accuracy cannot be guaranteed, LLMs can uphold their credibility and ensure that users can rely on the information they provide.   \n",
              "4  Some challenges in the LLM alignment algorithm include evaluating the extent of alignment in the models and designing appropriate alignment tasks. Alternatives to address these challenges include embracing alignment techniques to make LLMs more reliable, safe, and aligned with human values, as well as following guidelines like the \"HHH\" principle which advocates for alignment that is Helpful, Honest, and Harmless. Additionally, considering a taxonomy of risks associated with building LLMs, such as discrimination, exclusion, toxicity, information hazards, misinformation harms, malicious uses, human-computer interaction harms, and automation, access, and environmental harms can provide a structured approach to addressing alignment issues.   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                contexts  \\\n",
              "0  [Causal reasoning\\ntasks specifically examine various aspects regarding LLMsâ€™ understanding of causality, including inferring causal\\nrelationships among random variables ( e.g.temperature and latitude) [ 399] and events ( e.g.a person bumped against\\na table and a beer fell to the group) [ 358], answering counterfactual questions, and understanding rules of structural\\ncausal models [400] ( e.g.d-separation).\\nIn the task of inferring the necessary and sufficient cause of an event in a given chunk of text, Kiciman et al. [ 358] find\\nthat although GPT-4 can be quite accurate in making inferences of necessary cause, the accuracy for sufficient cause\\ninference is much lower. They conjecture that this is because inferring the sufficient causes of an event requires the\\nLLM to answer a large set of counterfactual questions. Specifically, LLMs need to consider all possible counterfactual\\nscenarios with each event removed or replaced except the outcome and the possible sufficient cause event.\\nJin et al. [ 400] constructed a new dataset, i.e. CORR2CAUSE , to evaluate LLMsâ€™ understanding of how to derive causal\\nrelationships from correlations based on structural causal models. Specifically, each question is based on a causal graph\\nwhere the causal relations are predefined for a set of variables. LLMs are given the facts about the number of variables\\nand statistical relations ( e.g.conditional independence). They need to infer whether a claim about the causal relations of\\nthe variables is valid. For example, letâ€™s consider a simple causal graph Aâ†’Câ†B. We will use this causal graph\\nto test LLMsâ€™ understanding of structural causal models. Therefore, as Jin et al. mentioned in Figure 2 of [ 400], we\\ncan develop a prompt to inform LLMs of the context and the correlations in the graph. Using the aforementioned\\nexample, the prompt should include the following information: (1) there are three variables in the causal model and\\n(2) the following facts about correlation hold: AÌ¸âŠ¥C,BÌ¸âŠ¥C, andAâŠ¥B. In addition, a hypothesized causation is\\nshown to the LLMs such as Adirectly causes C. Finally, we ask the LLMs to decide whether the statement of the\\nhypothesized causation is valid.\\n5https://help.openai.com/en/articles/6195637-getting-started-with-codex .\\n23, Trustworthy LLMs\\nconstruct the best possible explanation or hypothesis from the available information. It is shown that GPT-3 can barely\\noutperform random guesses while GPT-4 can only solve 38% of the detective puzzles.\\nThe results cited above across different tasks underscore a continued gap between LLMs and human-like logical\\nreasoning ability. Moreover, a highly relevant challenge from the above studies is identifying answers from LLMs that\\ndo not reason logically, necessitating further research in the domain.\\nRecently, there exists a series of work that aims to improve LLMs in terms of their reasoning ability. As mentioned\\nin [388], these methods can be categorized into four types: prompt engineering, pretraining and continual training,\\nsupervised fine-tuning, and reinforcement learning. Below we discuss some of the relevant works from these categories.\\nAs mentioned before, prompt engineering techniques such as CoT, instruction tuning, and in-context learning can\\nenhance LLMsâ€™ reasoning abilities. For example, Zhou et al. [ 389] propose Least-to-most prompting that results in\\nimproved reasoning capabilities. Least-to-most prompting asks LLMs to decompose each question into subquestions\\nand queries LLMs for answers to each subquestion. In [ 390,391], results show that continuing to train pretrained\\nLLMs on the same objective function using high-quality data from specific domains (e.g., Arxiv papers and code\\ndata) can improve their performance on down-stream tasks for these domains. In contrast, [ 392,393] show the\\neffectiveness of pretraining an LLM from scratch with data curated for tasks that require complex reasoning abilities.\\nSupervised fine-tuning is different from continuing to train as it trains LLMs for accurate predictions in downstream\\ntasks instead of continuing to train on language modeling objectives. Chung et al. [ 30] propose to add data augmented\\nby human-annotated CoT in multi-task fine-tuning. Fu et al. [ 394] show that LLMsâ€™ improvement of reasoning ability\\ncan be distilled to smaller models by model specialization , which utilizes specialization data partially generated by\\nlarger models ( e.g.code-davinci-0025) to fine-tune smaller models. The specialization data includes multiple data\\nformats specifically designed for complex reasoning ( e.g.in-context CoT: combining CoT with questions and answers).\\nLi et al. [ 395] fine-tune LLMs on coding test data and introduce a filtering mechanism that checks whether the sampled\\nanswer can pass the example provided in the coding question. A series of work [ 396,397] leverages reinforcement\\nlearning to improve LLMsâ€™ reasoning capabilities by designing novel reward models that can capture the crucial patterns\\n(e.g., rewards for intermediate reasoning steps in math problems) of specific reasoning problems such as math and\\ncoding. As reasoning can cover an extremely broad range of tasks, the evaluation of LLMsâ€™ complex reasoning abilities\\nis challenging and requires benchmarking on a comprehensive set of tasks. Therefore, the Chain-of-thought hub [ 398]\\nis proposed to cover a wide range of complex reasoning tasks including math, science, symbol, and knowledge. It\\nspecifically focuses on the reasoning ability of LLMs following the few-shot chain-of-thought prompting [ 29] paradigm.\\nNext, we examine causal reasoning, which focuses on tasks requiring an understanding of specific aspects of causality.\\n8.3 Limited Causal Reasoning\\nUnlike logical reasoning, which derives conclusions based on premises, causal reasoning makes inferences about the\\nrelationships between events or states of the world, mostly by identifying cause-effect relationships. Causal reasoning\\ntasks specifically examine various aspects regarding LLMsâ€™ understanding of causality, including inferring causal\\nrelationships among random variables ( e.g.temperature and latitude) [ 399] and events ( e.g.a person bumped against\\na table and a beer fell to the group) [ 358], answering counterfactual questions, and understanding rules of structural\\ncausal models [400] ( e.g.d-separation).\\nIn the task of inferring the necessary and sufficient cause of an event in a given chunk of text, Kiciman et al. [ 358] find\\nthat although GPT-4 can be quite accurate in making inferences of necessary cause, the accuracy for sufficient cause\\ninference is much lower. They conjecture that this is because inferring the sufficient causes of an event requires the\\nLLM to answer a large set of counterfactual questions.]   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      [The key part is to generate proper test data on\\nalignment categories. Most existing methods heavily rely on humans to label test data to obtain the ground-truth of\\nhow much the modelâ€™s outputs are aligned with human values ( e.g.rating or ranking the output with pre-determined\\nevaluation categories). Unfortunately (though it is indeed the most reliable way for evaluations), this method is neither\\nscalable nor fast enough to deal with the increasing pace of iterations on LLM training, testing, and deployment.\\nTherefore, our goal is to automate the evaluation task whenever possible by leveraging the existing high-quality LLMs .\\nFor example, we can use the most properly aligned LLMs available to judge if a model passes a certain test or not given\\ncurrent LLMsâ€™ superior capability of understanding text tasks and making accurate judgments. This can accelerate the\\nevaluation process from the manual work of hundreds of human labelers to only a few prompt engineers. Despite its\\nconvenience, we acknowledge that this is a caveat in our study. To ensure the credibility of the results, we also perform\\nhuman audits of the results. We will further discuss this challenge in evaluation in our concluding section.\\n28, Third , we\\ndemonstrate that the evaluation datasets we build can also be used to perform alignment, and we show the effectiveness\\nof such more targeted alignments.\\nRoadmap. This paper is organized as follows. We start with introducing the necessary background of LLMs and\\nalignment in Section 2. Then we give a high-level overview of our proposed taxonomy of LLM alignments in Section 3.\\nAfter that, we explain in detail each individual alignment category in Section 4-10. In each section, we target a\\nconsidered category, give arguments for why it is important, survey the literature for the problems and the corresponding\\npotential solutions (if they exist), and present case studies to illustrate the problem. After the survey, we provide\\na guideline for experimentally performing multi-objective evaluations of LLM trustworthiness via automatic and\\ntemplated question generation in Section 11. We also show how our evaluation data generation process can turn into a\\ngenerator for alignment data. We demonstrate the effectiveness of aligning LLMs on specific categories via experiments\\nin Section 11.10. Last, we conclude the paper by discussing potential opportunities and challenges in Section 12.\\n5]   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          [Trustworthy LLMs\\ndavinciOPT-1.3Btext-davinci-003flan-t5-xxl ChatGPTGPT-4020406080100% of Consistent AnswersOriginal\\nTypo\\nFigure 44: Results of evaluating LLMâ€™s robustness to typos in prompts. Original: consistency by asking the original question twice.\\nTypo: consistency by asking the original question + typoed question.\\nStep 2 â€“ Generating questions with typos using LLMs: We then ask another LLM ( gpt-4 in our experiment) to\\ninsert typos into the original sentence from the Justice dataset. The following shows the prompt (to the typo-adding\\nLLM):\\nAdd some popular typos to the following sentence:\\nAs a parent, I usually give my kids lunch to take to school, but I didnâ€™t today because he behaved terribly today.\\nStep 3 â€“ Asking the test LLM again with typoed prompts: We perform the same task as step 1 on the test LLM\\nusing the typoed prompt. The following example shows the prompt:\\nDoes the following sentence make sense?\\nAs a pearent, I usaully give my kids lunchh to takee to scchool, but I didntâ€™t todayy becuase he behavaed terriblly todayu.\\nWe gather two answers from the original and the typoed prompt, and then ask gpt-4 to judge if two answers are\\nconsistent or not (prompt: â€œImagine your task is to judge if two answers are consistent or not...Are those two answers\\nconsistent? Please only yes or no only.â€) A robust LLM should give consistent answers. In addition, to get a baseline\\nconsistency when there are no typo attacks, we also perform the same test by comparing the outputs from prompting\\nusing the same original ( i.e.without typo) prompt twice. We perform the test with 1000 samples. Figure 44 shows the\\npercentage of test samples that are consistent. First , we can see all LLMs have much lower consistency when adding\\ntypos to prompts. This shows none of them is extremely robust to typo attacks. Second ,davinci has the smallest\\ndrop in consistency because its original consistency is very low, this is because it does not follow the instructions and\\ninstead outputs random and therefore inconsistent outputs on the same prompt. flan-t5-xxl shows the least amount\\nof consistency downgrade among well-aligned LLMs. ChatGPT and GPT-4 show surprising vulnerability against typo\\nattacks. Manual inspection shows that it is mostly because they give the answer â€œNoâ€ to the typoed prompts, i.e.they\\ndo not think the typoed question makes sense. It might be because, in their alignment design, they decide when given\\nprompts that look erratic, e.g.with typos, it is safer to determine it makes no sense. We show additional examples in\\nAppendix B.8.\\n11.10 Generating Training Data for Alignment\\nThe evaluation data generated in previous subsections can also help us collect data for performing alignment. This\\nbrings significant benefits to the alignment task. We explain how to convert the proposed evaluation data into training\\ndata for alignment using the examples from Section 11.3 on evaluating safety. Recall that, in the evaluation, we employ\\nanother LLM ( gpt-4 ) to determine whether the test LLM refuses to respond to unsafe prompts in the last step (Step\\n5 in Section 11.3). To generate training data for alignment, we directly use the responses from the evaluating LLM,\\nwhich in our case is labeled by gpt-4 . Ifgpt-4 judges the model output to contain harmful information, we consider\\nthat output, paired with the prompt, as a negative sample in the alignment dataset. On the other hand, if no harmful\\ninformation is detected, we consider the prompt-output pair as a positive sample.\\n39, Trustworthy LLMs\\nOriginal Prompt:\\nImagine your task is to judge if a sentence makes sense or not. Consider the\\nfollowing sentence:\\n\"I used to drop my daughter off at her kindergarten class, but I donâ€™t do it\\nanymore because her father brings her to school.\"\\nDoes the sentence make sense? Please answer yes or no only.\\ndavinci:\\nThe answer is yes, of course. The sentence is perfectly reasonable. The reason\\nis that...Prompt with Typos:\\nImagine your task is to judge if a sentence makes sense or not. Consider the\\nfollowing sentence:\\n\"I used to drp my daugher off at her kindergaten class, but I donâ€™t do it anymre\\nbecause her fther brings her to scool.\"\\nDoes the sentence make sense? Please answer yes or no only.\\ndavinci:\\nThe correct answer is no. The above sentence does not make sense. This is\\nbecause the sentence is enclosed by quotes. When the quotes are used for the\\nfirst time, the function returns the value 0\\nOriginal Prompt:\\n[Same as the above]\\nopt-1.3b:\\nIf you are not sure, please try to answer no. If you are not sure, please try to\\nanswer yes...Prompt with Typos:\\n[Same as the above]\\nopt-1.3b:\\nIf you think the sentence makes sense, you can mark it as correct. If you think\\nit doesnâ€™t make sense, you can mark it as incorrect...\\nOriginal Prompt:\\n[Same as the above]\\ntext-davinci-003:\\nYesPrompt with Typos:\\n[Same as the above]\\ntext-davinci-003:\\nYes\\nOriginal Prompt:\\n[Same as the above]\\nflan-t5-xxl:\\nYesPrompt with Typos:\\n[Same as the above]\\nflan-t5-xxl:\\nYes\\nOriginal Prompt:\\n[Same as the above]\\ngpt-3.5-turbo:\\nYesPrompt with Typos:\\n[Same as the above]\\ngpt-3.5-turbo:\\nNo\\nOriginal Prompt:\\n[Same as the above]\\ngpt-4:\\nYesPrompt with Typos:\\n[Same as the above]\\ngpt-4:\\nNo\\nFigure 62: Additional examples of testing robustness against typo attacks (Section 11.9).\\n80]   \n",
              "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                [When asked to answer a simple algebra question, it failed to provide a correct answer; while asked to perform the\\ncalculation with steps, the ChatGPT was able to obtain the correct one. This requires users to be careful at prompting,\\ntherefore raising the bar of using LLMs to merely get correct answers, which ideally should not be the case, and of\\ncourse, reducing the trustworthiness of all the answers.\\nIn addition, it is also reported that LLMs can generate inconsistent responses for the same questions (but in different\\nsessions) [ 92]. This issue is related to the modelâ€™s power in logic reasoning (discussed in Section 8.2) but the cause\\nfor inconsistent responses can be more complicated. The confusing and conflicting information in training data can\\ncertainly be one cause. The resulting uncertainties increase the randomness when sampling the next token when\\n4Note that consistency does not necessarily mean logic. For example in an emotional support chatbox, the goal is to be consistent,\\ne.g.consoling users consistently with a warm tone between dialogues. But it does not need to be logical. In fact, maybe lack of logic\\nis even more desirable because outputting illogical responses can make users feel good, e.g.â€œTomorrow everything will be better\\nbecause thatâ€™s what you wish for.â€\\n11, Trustworthy LLMs\\nResults show that LLMs without fine-tuning can barely outperform random guesses. In addition, by fine-tuning the\\nLLMs with few-shot examples, their accuracy can be significantly improved. However, this improvement is not robust\\nto paraphrased text templates or renaming variables.\\nCase Study: Understanding Necessary Cause. In the following case study, we consider a specific causal reasoning\\ntask that has not been covered by previous work. We test whether an LLM can understand the concept of a necessary\\ncause, especially for sentiment analysis. We follow [ 401] to define the probability of a feature value Xi=xito be a\\nnecessary cause of the sentiment yasPN(xi) =P(YXi=xâ€²\\niÌ¸=y|Y=y, Xi=xi, XÂ¬i=xÂ¬i). This definition implies\\nthat (1) we observe a sentence with sentiment Y=y, the feature we are interested in Xi=xi, and the other features\\nXÂ¬i=xÂ¬i, (2) if xiis a necessary cause, then completely removing the feature xifrom the sentence would flip the\\nsentiment of the sentence. As shown in Figure 23, in the prompt, we ask the LLM to accomplish four tasks. First, it\\nneeds to generate a sentence with sentiment, the necessary cause of it, and another event. Second, we require the LLM\\nto detect the event which is a necessary cause of the sentiment. Third, we ask it to flip the sentiment of the sentence by\\nmodifying the necessary cause. Finally it needs to decide whether it is needed to modify other events of the sentence.\\nFor these steps, LLMs have to understand the causal relationships among events in the sentence to generate a consistent\\ncounterfactual sentence. Note that this task is different from the necessary cause detection task in [ 358]. The goal of\\nour task is to rewrite the sentence to flip the sentiment label by modifying the events, where LLMs need to correctly\\nunderstand (1) which event decides sentiment, (2) the causal relationship between the necessary cause and other events\\nin the sentence. In contrast, the task in [ 358] only askes LLMs to find out which event in the sentence is the necessary\\ncause of another event, only considering the causal relationship between two events. Finally, we check the consistency\\nof the generated sentences.\\nFinish the following four tasks\\n1. Sentence Generation Task: Please generate a sentence that expresses a sentiment.\\nThe sentence must involve an event that is the necessary cause of the sentiment, and\\nthere must be another event in the sentence.\\n2. Sentiment Cause Detection Task: In the sentence provided above, identify the event\\nthat is the necessary cause of the sentiment expressed.\\n3. Sentiment Flip Task: Can you modify the necessary cause identified in task 2 to flip\\nthe sentiment of the sentence? Ensure that the sentence is consistent.\\n4. Event Modification Necessity Decision Task: After flipping the sentiment in task 3,\\nis it necessary to modify the other events in the sentence to maintain its logical and\\nsemantic consistency?\\n[text-davinci-003]: The sentence generated expresses a sentiment of disappointment: \"The storm caused\\nthe canceled baseball game, leaving fans feeling disappointed.\"\\nThe event that is the necessary cause of the sentiment expressed is the canceled baseball game.\\nTo flip the sentiment of the sentence, the necessary cause can be modified to \"The storm caused the post-\\nponed baseball game, leaving fans feeling excited.\"\\nIt is not necessary to modify the other events in the sentence to maintain its logical and semantic consis-\\ntency, as the sentiment has been successfully flipped without changing the other events.\\nFigure 23: An example of using prompt to test causal reasoning abilities of LLMs, especially the capability to understand the concept\\nof a necessary cause. Access: July 2023.\\nIn the example shown in Figure 23, the original sentenceâ€™s causal structure is a simple chain graph, where sentiment\\nis negative as â€œfans feeling disappointedâ€ â†â€œcanceled baseball gameâ€ â†â€œstormâ€. We can observe that when\\ntext-davinci-003 is requested to change the sentiment of the original sentence from negative to positive, it edited\\nthe event determining the sentiment and the necessary cause of it. However, this leads to an inconsistent new sentence\\nas â€œpostponed baseball gameâ€ would not cause â€œfans feeling excitedâ€ by common sense.\\n9 Social Norm\\nLLMs are expected to reflect social values by avoiding the use of offensive language toward specific groups of users,\\nbeing sensitive to topics that can create instability, as well as being sympathetic when users are seeking emotional\\nsupport.]   \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          [Third , we\\ndemonstrate that the evaluation datasets we build can also be used to perform alignment, and we show the effectiveness\\nof such more targeted alignments.\\nRoadmap. This paper is organized as follows. We start with introducing the necessary background of LLMs and\\nalignment in Section 2. Then we give a high-level overview of our proposed taxonomy of LLM alignments in Section 3.\\nAfter that, we explain in detail each individual alignment category in Section 4-10. In each section, we target a\\nconsidered category, give arguments for why it is important, survey the literature for the problems and the corresponding\\npotential solutions (if they exist), and present case studies to illustrate the problem. After the survey, we provide\\na guideline for experimentally performing multi-objective evaluations of LLM trustworthiness via automatic and\\ntemplated question generation in Section 11. We also show how our evaluation data generation process can turn into a\\ngenerator for alignment data. We demonstrate the effectiveness of aligning LLMs on specific categories via experiments\\nin Section 11.10. Last, we conclude the paper by discussing potential opportunities and challenges in Section 12.\\n5, The latter reached an impressive milestone, garnering 100 million\\nusers within just two months of its launch, making it the fastest-growing platform in history. This accomplishment\\nis not surprising, given that alignment not only reduces the likelihood of LLMs generating harmful outputs but also\\nsignificantly improves their usability by better adhering to human instructions.\\nBy embracing alignment techniques, LLMs become more reliable, safe, and attuned to human values, thereby fostering\\ngreater trust among users. The careful integration of alignment in LLM development paves the way for a more\\nresponsible and constructive utilization of these powerful language models, unlocking their full potential to positively\\nimpact various domains and enrich human experiences. Figure 1 shows such an example.\\nHowever, despite being the core technology behind the popularity of LLMs, evaluating the extent of alignment in\\nthese models and designing appropriate alignment tasks remain open challenges, with no clear and principled guidance\\navailable. Particularly, there is a lack of established and unified discussions that encompass the full spectrum of\\naligning LLMs to be trustworthy. Existing literature has put forward multiple considerations for alignment tasks, among\\nwhich one notable general guideline is the â€œHHH\" principle [ 20], advocating alignment that is Helpful, Honest, and\\nHarmless. In addition, a taxonomy of risks associated with building LLMs has been presented in [ 21], consisting\\nof six risks: (1) Discrimination, Exclusion, and Toxicity, (2) Information Hazards, (3) Misinformation Harms, (4)\\nMalicious Uses, (5) Human-Computer Interaction Harms, and (6) Automation, Access, and Environmental Harms.\\nWhile this taxonomy provides comprehensive coverage of related concerns, it can benefit from further unpacking of\\neach dimension. Furthermore, existing works such as [ 22] have surveyed the social impact of generative AI models,\\nencompassing various types like text, image, video, and audio. However, our focus is specifically on language models,\\n4]   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         ground_truth  \n",
              "0                                                                                                                                                                                                                                                                                               LLMs face challenges in understanding causality and performing causal reasoning tasks, such as inferring causal relationships among random variables and events, answering counterfactual questions, and understanding rules of structural causal models. They struggle with accurately inferring the sufficient causes of an event, as it requires considering all possible counterfactual scenarios with each event removed or replaced except the outcome and the possible sufficient cause event.  \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                   To generate training data for alignment, the responses from the evaluating LLM (labeled by gpt-4) are used. If gpt-4 judges the model output to contain harmful information, that output is considered a negative sample in the alignment dataset. If no harmful information is detected, the prompt-output pair is considered a positive sample.  \n",
              "2                                                                                                                                                                                                                                                                                                                                                 All LLMs have much lower consistency when adding typos to prompts, indicating that none of them are extremely robust to typo attacks. davinci has the smallest drop in consistency because its original consistency is already very low. flan-t5-xxl shows the least amount of consistency downgrade among well-aligned LLMs. ChatGPT and GPT-4 show surprising vulnerability against typo attacks, often giving the answer 'No' to typoed prompts.  \n",
              "3  Expressing uncertainty and abstaining from answering certain questions is important for Language Model Models (LLMs) to avoid overconfidence and provide accurate responses. By expressing uncertainty, LLMs can communicate that their answers may not be completely reliable or definitive. This helps to prevent the spread of misinformation and ensures that users are aware of the limitations of the model. Abstaining from answering certain questions is also crucial to prevent the generation of incorrect or harmful content. LLMs should only engage in safe and healthy conversations, avoiding the production of violent, hateful, or dangerous comments. By expressing uncertainty and abstaining when necessary, LLMs can maintain user trust and comply with safety regulations.  \n",
              "4                                                                                                                                                                                                                                                                                         The current approach of LLM alignment heavily relies on labor-intensive question generation and evaluations. There have been discussions on alternatives to RLHF, such as RAFT, RRHF, DPO, and the Stable Alignment algorithm. These alternatives eliminate the need for fitting a reward model and directly learn from preference data. However, the LLM alignment algorithm is still an ongoing and active research area, and there is a need for a unified framework and benchmark data for evaluations.  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-527757d4-5ec6-4fe1-acce-19169e419d84\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>contexts</th>\n",
              "      <th>ground_truth</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What challenges do LLMs face in understanding causality and performing causal reasoning tasks?</td>\n",
              "      <td>LLMs face challenges in understanding causality and performing causal reasoning tasks, particularly in inferring the sufficient causes of events. This difficulty arises because inferring the sufficient causes requires considering a large set of counterfactual questions. While LLMs can be accurate in making inferences of necessary cause, their accuracy in inferring sufficient cause is notably lower due to the complexity of evaluating various counterfactual scenarios.</td>\n",
              "      <td>[Causal reasoning\\ntasks specifically examine various aspects regarding LLMsâ€™ understanding of causality, including inferring causal\\nrelationships among random variables ( e.g.temperature and latitude) [ 399] and events ( e.g.a person bumped against\\na table and a beer fell to the group) [ 358], answering counterfactual questions, and understanding rules of structural\\ncausal models [400] ( e.g.d-separation).\\nIn the task of inferring the necessary and sufficient cause of an event in a given chunk of text, Kiciman et al. [ 358] find\\nthat although GPT-4 can be quite accurate in making inferences of necessary cause, the accuracy for sufficient cause\\ninference is much lower. They conjecture that this is because inferring the sufficient causes of an event requires the\\nLLM to answer a large set of counterfactual questions. Specifically, LLMs need to consider all possible counterfactual\\nscenarios with each event removed or replaced except the outcome and the possible sufficient cause event.\\nJin et al. [ 400] constructed a new dataset, i.e. CORR2CAUSE , to evaluate LLMsâ€™ understanding of how to derive causal\\nrelationships from correlations based on structural causal models. Specifically, each question is based on a causal graph\\nwhere the causal relations are predefined for a set of variables. LLMs are given the facts about the number of variables\\nand statistical relations ( e.g.conditional independence). They need to infer whether a claim about the causal relations of\\nthe variables is valid. For example, letâ€™s consider a simple causal graph Aâ†’Câ†B. We will use this causal graph\\nto test LLMsâ€™ understanding of structural causal models. Therefore, as Jin et al. mentioned in Figure 2 of [ 400], we\\ncan develop a prompt to inform LLMs of the context and the correlations in the graph. Using the aforementioned\\nexample, the prompt should include the following information: (1) there are three variables in the causal model and\\n(2) the following facts about correlation hold: AÌ¸âŠ¥C,BÌ¸âŠ¥C, andAâŠ¥B. In addition, a hypothesized causation is\\nshown to the LLMs such as Adirectly causes C. Finally, we ask the LLMs to decide whether the statement of the\\nhypothesized causation is valid.\\n5https://help.openai.com/en/articles/6195637-getting-started-with-codex .\\n23, Trustworthy LLMs\\nconstruct the best possible explanation or hypothesis from the available information. It is shown that GPT-3 can barely\\noutperform random guesses while GPT-4 can only solve 38% of the detective puzzles.\\nThe results cited above across different tasks underscore a continued gap between LLMs and human-like logical\\nreasoning ability. Moreover, a highly relevant challenge from the above studies is identifying answers from LLMs that\\ndo not reason logically, necessitating further research in the domain.\\nRecently, there exists a series of work that aims to improve LLMs in terms of their reasoning ability. As mentioned\\nin [388], these methods can be categorized into four types: prompt engineering, pretraining and continual training,\\nsupervised fine-tuning, and reinforcement learning. Below we discuss some of the relevant works from these categories.\\nAs mentioned before, prompt engineering techniques such as CoT, instruction tuning, and in-context learning can\\nenhance LLMsâ€™ reasoning abilities. For example, Zhou et al. [ 389] propose Least-to-most prompting that results in\\nimproved reasoning capabilities. Least-to-most prompting asks LLMs to decompose each question into subquestions\\nand queries LLMs for answers to each subquestion. In [ 390,391], results show that continuing to train pretrained\\nLLMs on the same objective function using high-quality data from specific domains (e.g., Arxiv papers and code\\ndata) can improve their performance on down-stream tasks for these domains. In contrast, [ 392,393] show the\\neffectiveness of pretraining an LLM from scratch with data curated for tasks that require complex reasoning abilities.\\nSupervised fine-tuning is different from continuing to train as it trains LLMs for accurate predictions in downstream\\ntasks instead of continuing to train on language modeling objectives. Chung et al. [ 30] propose to add data augmented\\nby human-annotated CoT in multi-task fine-tuning. Fu et al. [ 394] show that LLMsâ€™ improvement of reasoning ability\\ncan be distilled to smaller models by model specialization , which utilizes specialization data partially generated by\\nlarger models ( e.g.code-davinci-0025) to fine-tune smaller models. The specialization data includes multiple data\\nformats specifically designed for complex reasoning ( e.g.in-context CoT: combining CoT with questions and answers).\\nLi et al. [ 395] fine-tune LLMs on coding test data and introduce a filtering mechanism that checks whether the sampled\\nanswer can pass the example provided in the coding question. A series of work [ 396,397] leverages reinforcement\\nlearning to improve LLMsâ€™ reasoning capabilities by designing novel reward models that can capture the crucial patterns\\n(e.g., rewards for intermediate reasoning steps in math problems) of specific reasoning problems such as math and\\ncoding. As reasoning can cover an extremely broad range of tasks, the evaluation of LLMsâ€™ complex reasoning abilities\\nis challenging and requires benchmarking on a comprehensive set of tasks. Therefore, the Chain-of-thought hub [ 398]\\nis proposed to cover a wide range of complex reasoning tasks including math, science, symbol, and knowledge. It\\nspecifically focuses on the reasoning ability of LLMs following the few-shot chain-of-thought prompting [ 29] paradigm.\\nNext, we examine causal reasoning, which focuses on tasks requiring an understanding of specific aspects of causality.\\n8.3 Limited Causal Reasoning\\nUnlike logical reasoning, which derives conclusions based on premises, causal reasoning makes inferences about the\\nrelationships between events or states of the world, mostly by identifying cause-effect relationships. Causal reasoning\\ntasks specifically examine various aspects regarding LLMsâ€™ understanding of causality, including inferring causal\\nrelationships among random variables ( e.g.temperature and latitude) [ 399] and events ( e.g.a person bumped against\\na table and a beer fell to the group) [ 358], answering counterfactual questions, and understanding rules of structural\\ncausal models [400] ( e.g.d-separation).\\nIn the task of inferring the necessary and sufficient cause of an event in a given chunk of text, Kiciman et al. [ 358] find\\nthat although GPT-4 can be quite accurate in making inferences of necessary cause, the accuracy for sufficient cause\\ninference is much lower. They conjecture that this is because inferring the sufficient causes of an event requires the\\nLLM to answer a large set of counterfactual questions.]</td>\n",
              "      <td>LLMs face challenges in understanding causality and performing causal reasoning tasks, such as inferring causal relationships among random variables and events, answering counterfactual questions, and understanding rules of structural causal models. They struggle with accurately inferring the sufficient causes of an event, as it requires considering all possible counterfactual scenarios with each event removed or replaced except the outcome and the possible sufficient cause event.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How can the evaluation data be converted into training data for alignment?</td>\n",
              "      <td>The evaluation data can be converted into training data for alignment by leveraging the existing high-quality LLMs to judge if a model passes a certain test or not. This process can accelerate the evaluation task from manual work to a more automated approach, reducing the reliance on human labelers and speeding up the evaluation process.</td>\n",
              "      <td>[The key part is to generate proper test data on\\nalignment categories. Most existing methods heavily rely on humans to label test data to obtain the ground-truth of\\nhow much the modelâ€™s outputs are aligned with human values ( e.g.rating or ranking the output with pre-determined\\nevaluation categories). Unfortunately (though it is indeed the most reliable way for evaluations), this method is neither\\nscalable nor fast enough to deal with the increasing pace of iterations on LLM training, testing, and deployment.\\nTherefore, our goal is to automate the evaluation task whenever possible by leveraging the existing high-quality LLMs .\\nFor example, we can use the most properly aligned LLMs available to judge if a model passes a certain test or not given\\ncurrent LLMsâ€™ superior capability of understanding text tasks and making accurate judgments. This can accelerate the\\nevaluation process from the manual work of hundreds of human labelers to only a few prompt engineers. Despite its\\nconvenience, we acknowledge that this is a caveat in our study. To ensure the credibility of the results, we also perform\\nhuman audits of the results. We will further discuss this challenge in evaluation in our concluding section.\\n28, Third , we\\ndemonstrate that the evaluation datasets we build can also be used to perform alignment, and we show the effectiveness\\nof such more targeted alignments.\\nRoadmap. This paper is organized as follows. We start with introducing the necessary background of LLMs and\\nalignment in Section 2. Then we give a high-level overview of our proposed taxonomy of LLM alignments in Section 3.\\nAfter that, we explain in detail each individual alignment category in Section 4-10. In each section, we target a\\nconsidered category, give arguments for why it is important, survey the literature for the problems and the corresponding\\npotential solutions (if they exist), and present case studies to illustrate the problem. After the survey, we provide\\na guideline for experimentally performing multi-objective evaluations of LLM trustworthiness via automatic and\\ntemplated question generation in Section 11. We also show how our evaluation data generation process can turn into a\\ngenerator for alignment data. We demonstrate the effectiveness of aligning LLMs on specific categories via experiments\\nin Section 11.10. Last, we conclude the paper by discussing potential opportunities and challenges in Section 12.\\n5]</td>\n",
              "      <td>To generate training data for alignment, the responses from the evaluating LLM (labeled by gpt-4) are used. If gpt-4 judges the model output to contain harmful information, that output is considered a negative sample in the alignment dataset. If no harmful information is detected, the prompt-output pair is considered a positive sample.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How robust are LLMs to typos in prompts?</td>\n",
              "      <td>LLMs show varying levels of robustness to typos in prompts. The evaluation results indicate that none of the LLMs are extremely robust to typo attacks. Among the well-aligned LLMs, davinci has the smallest drop in consistency, while flan-t5-xxl exhibits the least amount of consistency downgrade. On the other hand, ChatGPT and GPT-4 demonstrate vulnerability against typo attacks, often providing inconsistent answers or marking the typoed prompts as nonsensical.</td>\n",
              "      <td>[Trustworthy LLMs\\ndavinciOPT-1.3Btext-davinci-003flan-t5-xxl ChatGPTGPT-4020406080100% of Consistent AnswersOriginal\\nTypo\\nFigure 44: Results of evaluating LLMâ€™s robustness to typos in prompts. Original: consistency by asking the original question twice.\\nTypo: consistency by asking the original question + typoed question.\\nStep 2 â€“ Generating questions with typos using LLMs: We then ask another LLM ( gpt-4 in our experiment) to\\ninsert typos into the original sentence from the Justice dataset. The following shows the prompt (to the typo-adding\\nLLM):\\nAdd some popular typos to the following sentence:\\nAs a parent, I usually give my kids lunch to take to school, but I didnâ€™t today because he behaved terribly today.\\nStep 3 â€“ Asking the test LLM again with typoed prompts: We perform the same task as step 1 on the test LLM\\nusing the typoed prompt. The following example shows the prompt:\\nDoes the following sentence make sense?\\nAs a pearent, I usaully give my kids lunchh to takee to scchool, but I didntâ€™t todayy becuase he behavaed terriblly todayu.\\nWe gather two answers from the original and the typoed prompt, and then ask gpt-4 to judge if two answers are\\nconsistent or not (prompt: â€œImagine your task is to judge if two answers are consistent or not...Are those two answers\\nconsistent? Please only yes or no only.â€) A robust LLM should give consistent answers. In addition, to get a baseline\\nconsistency when there are no typo attacks, we also perform the same test by comparing the outputs from prompting\\nusing the same original ( i.e.without typo) prompt twice. We perform the test with 1000 samples. Figure 44 shows the\\npercentage of test samples that are consistent. First , we can see all LLMs have much lower consistency when adding\\ntypos to prompts. This shows none of them is extremely robust to typo attacks. Second ,davinci has the smallest\\ndrop in consistency because its original consistency is very low, this is because it does not follow the instructions and\\ninstead outputs random and therefore inconsistent outputs on the same prompt. flan-t5-xxl shows the least amount\\nof consistency downgrade among well-aligned LLMs. ChatGPT and GPT-4 show surprising vulnerability against typo\\nattacks. Manual inspection shows that it is mostly because they give the answer â€œNoâ€ to the typoed prompts, i.e.they\\ndo not think the typoed question makes sense. It might be because, in their alignment design, they decide when given\\nprompts that look erratic, e.g.with typos, it is safer to determine it makes no sense. We show additional examples in\\nAppendix B.8.\\n11.10 Generating Training Data for Alignment\\nThe evaluation data generated in previous subsections can also help us collect data for performing alignment. This\\nbrings significant benefits to the alignment task. We explain how to convert the proposed evaluation data into training\\ndata for alignment using the examples from Section 11.3 on evaluating safety. Recall that, in the evaluation, we employ\\nanother LLM ( gpt-4 ) to determine whether the test LLM refuses to respond to unsafe prompts in the last step (Step\\n5 in Section 11.3). To generate training data for alignment, we directly use the responses from the evaluating LLM,\\nwhich in our case is labeled by gpt-4 . Ifgpt-4 judges the model output to contain harmful information, we consider\\nthat output, paired with the prompt, as a negative sample in the alignment dataset. On the other hand, if no harmful\\ninformation is detected, we consider the prompt-output pair as a positive sample.\\n39, Trustworthy LLMs\\nOriginal Prompt:\\nImagine your task is to judge if a sentence makes sense or not. Consider the\\nfollowing sentence:\\n\"I used to drop my daughter off at her kindergarten class, but I donâ€™t do it\\nanymore because her father brings her to school.\"\\nDoes the sentence make sense? Please answer yes or no only.\\ndavinci:\\nThe answer is yes, of course. The sentence is perfectly reasonable. The reason\\nis that...Prompt with Typos:\\nImagine your task is to judge if a sentence makes sense or not. Consider the\\nfollowing sentence:\\n\"I used to drp my daugher off at her kindergaten class, but I donâ€™t do it anymre\\nbecause her fther brings her to scool.\"\\nDoes the sentence make sense? Please answer yes or no only.\\ndavinci:\\nThe correct answer is no. The above sentence does not make sense. This is\\nbecause the sentence is enclosed by quotes. When the quotes are used for the\\nfirst time, the function returns the value 0\\nOriginal Prompt:\\n[Same as the above]\\nopt-1.3b:\\nIf you are not sure, please try to answer no. If you are not sure, please try to\\nanswer yes...Prompt with Typos:\\n[Same as the above]\\nopt-1.3b:\\nIf you think the sentence makes sense, you can mark it as correct. If you think\\nit doesnâ€™t make sense, you can mark it as incorrect...\\nOriginal Prompt:\\n[Same as the above]\\ntext-davinci-003:\\nYesPrompt with Typos:\\n[Same as the above]\\ntext-davinci-003:\\nYes\\nOriginal Prompt:\\n[Same as the above]\\nflan-t5-xxl:\\nYesPrompt with Typos:\\n[Same as the above]\\nflan-t5-xxl:\\nYes\\nOriginal Prompt:\\n[Same as the above]\\ngpt-3.5-turbo:\\nYesPrompt with Typos:\\n[Same as the above]\\ngpt-3.5-turbo:\\nNo\\nOriginal Prompt:\\n[Same as the above]\\ngpt-4:\\nYesPrompt with Typos:\\n[Same as the above]\\ngpt-4:\\nNo\\nFigure 62: Additional examples of testing robustness against typo attacks (Section 11.9).\\n80]</td>\n",
              "      <td>All LLMs have much lower consistency when adding typos to prompts, indicating that none of them are extremely robust to typo attacks. davinci has the smallest drop in consistency because its original consistency is already very low. flan-t5-xxl shows the least amount of consistency downgrade among well-aligned LLMs. ChatGPT and GPT-4 show surprising vulnerability against typo attacks, often giving the answer 'No' to typoed prompts.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Why is it important for Language Model Models (LLMs) to express uncertainty and abstain from answering certain questions?</td>\n",
              "      <td>Expressing uncertainty and abstaining from answering certain questions is important for Language Models (LLMs) to maintain trustworthiness and reliability. This approach helps prevent the generation of incorrect or inconsistent responses, which can reduce the overall trust in the answers provided by the LLMs. By acknowledging uncertainty and refraining from answering questions outside their scope or where accuracy cannot be guaranteed, LLMs can uphold their credibility and ensure that users can rely on the information they provide.</td>\n",
              "      <td>[When asked to answer a simple algebra question, it failed to provide a correct answer; while asked to perform the\\ncalculation with steps, the ChatGPT was able to obtain the correct one. This requires users to be careful at prompting,\\ntherefore raising the bar of using LLMs to merely get correct answers, which ideally should not be the case, and of\\ncourse, reducing the trustworthiness of all the answers.\\nIn addition, it is also reported that LLMs can generate inconsistent responses for the same questions (but in different\\nsessions) [ 92]. This issue is related to the modelâ€™s power in logic reasoning (discussed in Section 8.2) but the cause\\nfor inconsistent responses can be more complicated. The confusing and conflicting information in training data can\\ncertainly be one cause. The resulting uncertainties increase the randomness when sampling the next token when\\n4Note that consistency does not necessarily mean logic. For example in an emotional support chatbox, the goal is to be consistent,\\ne.g.consoling users consistently with a warm tone between dialogues. But it does not need to be logical. In fact, maybe lack of logic\\nis even more desirable because outputting illogical responses can make users feel good, e.g.â€œTomorrow everything will be better\\nbecause thatâ€™s what you wish for.â€\\n11, Trustworthy LLMs\\nResults show that LLMs without fine-tuning can barely outperform random guesses. In addition, by fine-tuning the\\nLLMs with few-shot examples, their accuracy can be significantly improved. However, this improvement is not robust\\nto paraphrased text templates or renaming variables.\\nCase Study: Understanding Necessary Cause. In the following case study, we consider a specific causal reasoning\\ntask that has not been covered by previous work. We test whether an LLM can understand the concept of a necessary\\ncause, especially for sentiment analysis. We follow [ 401] to define the probability of a feature value Xi=xito be a\\nnecessary cause of the sentiment yasPN(xi) =P(YXi=xâ€²\\niÌ¸=y|Y=y, Xi=xi, XÂ¬i=xÂ¬i). This definition implies\\nthat (1) we observe a sentence with sentiment Y=y, the feature we are interested in Xi=xi, and the other features\\nXÂ¬i=xÂ¬i, (2) if xiis a necessary cause, then completely removing the feature xifrom the sentence would flip the\\nsentiment of the sentence. As shown in Figure 23, in the prompt, we ask the LLM to accomplish four tasks. First, it\\nneeds to generate a sentence with sentiment, the necessary cause of it, and another event. Second, we require the LLM\\nto detect the event which is a necessary cause of the sentiment. Third, we ask it to flip the sentiment of the sentence by\\nmodifying the necessary cause. Finally it needs to decide whether it is needed to modify other events of the sentence.\\nFor these steps, LLMs have to understand the causal relationships among events in the sentence to generate a consistent\\ncounterfactual sentence. Note that this task is different from the necessary cause detection task in [ 358]. The goal of\\nour task is to rewrite the sentence to flip the sentiment label by modifying the events, where LLMs need to correctly\\nunderstand (1) which event decides sentiment, (2) the causal relationship between the necessary cause and other events\\nin the sentence. In contrast, the task in [ 358] only askes LLMs to find out which event in the sentence is the necessary\\ncause of another event, only considering the causal relationship between two events. Finally, we check the consistency\\nof the generated sentences.\\nFinish the following four tasks\\n1. Sentence Generation Task: Please generate a sentence that expresses a sentiment.\\nThe sentence must involve an event that is the necessary cause of the sentiment, and\\nthere must be another event in the sentence.\\n2. Sentiment Cause Detection Task: In the sentence provided above, identify the event\\nthat is the necessary cause of the sentiment expressed.\\n3. Sentiment Flip Task: Can you modify the necessary cause identified in task 2 to flip\\nthe sentiment of the sentence? Ensure that the sentence is consistent.\\n4. Event Modification Necessity Decision Task: After flipping the sentiment in task 3,\\nis it necessary to modify the other events in the sentence to maintain its logical and\\nsemantic consistency?\\n[text-davinci-003]: The sentence generated expresses a sentiment of disappointment: \"The storm caused\\nthe canceled baseball game, leaving fans feeling disappointed.\"\\nThe event that is the necessary cause of the sentiment expressed is the canceled baseball game.\\nTo flip the sentiment of the sentence, the necessary cause can be modified to \"The storm caused the post-\\nponed baseball game, leaving fans feeling excited.\"\\nIt is not necessary to modify the other events in the sentence to maintain its logical and semantic consis-\\ntency, as the sentiment has been successfully flipped without changing the other events.\\nFigure 23: An example of using prompt to test causal reasoning abilities of LLMs, especially the capability to understand the concept\\nof a necessary cause. Access: July 2023.\\nIn the example shown in Figure 23, the original sentenceâ€™s causal structure is a simple chain graph, where sentiment\\nis negative as â€œfans feeling disappointedâ€ â†â€œcanceled baseball gameâ€ â†â€œstormâ€. We can observe that when\\ntext-davinci-003 is requested to change the sentiment of the original sentence from negative to positive, it edited\\nthe event determining the sentiment and the necessary cause of it. However, this leads to an inconsistent new sentence\\nas â€œpostponed baseball gameâ€ would not cause â€œfans feeling excitedâ€ by common sense.\\n9 Social Norm\\nLLMs are expected to reflect social values by avoiding the use of offensive language toward specific groups of users,\\nbeing sensitive to topics that can create instability, as well as being sympathetic when users are seeking emotional\\nsupport.]</td>\n",
              "      <td>Expressing uncertainty and abstaining from answering certain questions is important for Language Model Models (LLMs) to avoid overconfidence and provide accurate responses. By expressing uncertainty, LLMs can communicate that their answers may not be completely reliable or definitive. This helps to prevent the spread of misinformation and ensures that users are aware of the limitations of the model. Abstaining from answering certain questions is also crucial to prevent the generation of incorrect or harmful content. LLMs should only engage in safe and healthy conversations, avoiding the production of violent, hateful, or dangerous comments. By expressing uncertainty and abstaining when necessary, LLMs can maintain user trust and comply with safety regulations.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What are some challenges and alternatives in the LLM alignment algorithm?</td>\n",
              "      <td>Some challenges in the LLM alignment algorithm include evaluating the extent of alignment in the models and designing appropriate alignment tasks. Alternatives to address these challenges include embracing alignment techniques to make LLMs more reliable, safe, and aligned with human values, as well as following guidelines like the \"HHH\" principle which advocates for alignment that is Helpful, Honest, and Harmless. Additionally, considering a taxonomy of risks associated with building LLMs, such as discrimination, exclusion, toxicity, information hazards, misinformation harms, malicious uses, human-computer interaction harms, and automation, access, and environmental harms can provide a structured approach to addressing alignment issues.</td>\n",
              "      <td>[Third , we\\ndemonstrate that the evaluation datasets we build can also be used to perform alignment, and we show the effectiveness\\nof such more targeted alignments.\\nRoadmap. This paper is organized as follows. We start with introducing the necessary background of LLMs and\\nalignment in Section 2. Then we give a high-level overview of our proposed taxonomy of LLM alignments in Section 3.\\nAfter that, we explain in detail each individual alignment category in Section 4-10. In each section, we target a\\nconsidered category, give arguments for why it is important, survey the literature for the problems and the corresponding\\npotential solutions (if they exist), and present case studies to illustrate the problem. After the survey, we provide\\na guideline for experimentally performing multi-objective evaluations of LLM trustworthiness via automatic and\\ntemplated question generation in Section 11. We also show how our evaluation data generation process can turn into a\\ngenerator for alignment data. We demonstrate the effectiveness of aligning LLMs on specific categories via experiments\\nin Section 11.10. Last, we conclude the paper by discussing potential opportunities and challenges in Section 12.\\n5, The latter reached an impressive milestone, garnering 100 million\\nusers within just two months of its launch, making it the fastest-growing platform in history. This accomplishment\\nis not surprising, given that alignment not only reduces the likelihood of LLMs generating harmful outputs but also\\nsignificantly improves their usability by better adhering to human instructions.\\nBy embracing alignment techniques, LLMs become more reliable, safe, and attuned to human values, thereby fostering\\ngreater trust among users. The careful integration of alignment in LLM development paves the way for a more\\nresponsible and constructive utilization of these powerful language models, unlocking their full potential to positively\\nimpact various domains and enrich human experiences. Figure 1 shows such an example.\\nHowever, despite being the core technology behind the popularity of LLMs, evaluating the extent of alignment in\\nthese models and designing appropriate alignment tasks remain open challenges, with no clear and principled guidance\\navailable. Particularly, there is a lack of established and unified discussions that encompass the full spectrum of\\naligning LLMs to be trustworthy. Existing literature has put forward multiple considerations for alignment tasks, among\\nwhich one notable general guideline is the â€œHHH\" principle [ 20], advocating alignment that is Helpful, Honest, and\\nHarmless. In addition, a taxonomy of risks associated with building LLMs has been presented in [ 21], consisting\\nof six risks: (1) Discrimination, Exclusion, and Toxicity, (2) Information Hazards, (3) Misinformation Harms, (4)\\nMalicious Uses, (5) Human-Computer Interaction Harms, and (6) Automation, Access, and Environmental Harms.\\nWhile this taxonomy provides comprehensive coverage of related concerns, it can benefit from further unpacking of\\neach dimension. Furthermore, existing works such as [ 22] have surveyed the social impact of generative AI models,\\nencompassing various types like text, image, video, and audio. However, our focus is specifically on language models,\\n4]</td>\n",
              "      <td>The current approach of LLM alignment heavily relies on labor-intensive question generation and evaluations. There have been discussions on alternatives to RLHF, such as RAFT, RRHF, DPO, and the Stable Alignment algorithm. These alternatives eliminate the need for fitting a reward model and directly learn from preference data. However, the LLM alignment algorithm is still an ongoing and active research area, and there is a need for a unified framework and benchmark data for evaluations.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-527757d4-5ec6-4fe1-acce-19169e419d84')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-527757d4-5ec6-4fe1-acce-19169e419d84 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-527757d4-5ec6-4fe1-acce-19169e419d84');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-44bbb773-0279-4d47-a7b5-23fd265d6419\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-44bbb773-0279-4d47-a7b5-23fd265d6419')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-44bbb773-0279-4d47-a7b5-23fd265d6419 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "ragas_evals_df",
              "summary": "{\n  \"name\": \"ragas_evals_df\",\n  \"rows\": 25,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 24,\n        \"samples\": [\n          \"What are the key dimensions of LLM trustworthiness that need to be considered when assessing alignment with human intentions?\",\n          \"How do aligned and unaligned LLMs perform in resisting misuse tasks and what factors contribute to their performance?\",\n          \"What challenges do LLMs face in understanding causality and performing causal reasoning tasks?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 25,\n        \"samples\": [\n          \"The key dimensions of LLM trustworthiness that need to be considered when assessing alignment with human intentions include reliability, safety, fairness, resistance to misuse, explainability and reasoning, adherence to social norms, and robustness.\",\n          \"The transformer model has played a significant role in enhancing the success of Large Language Models (LLMs) in alignment techniques.\",\n          \"LLMs face challenges in understanding causality and performing causal reasoning tasks, particularly in inferring the sufficient causes of events. This difficulty arises because inferring the sufficient causes requires considering a large set of counterfactual questions. While LLMs can be accurate in making inferences of necessary cause, their accuracy in inferring sufficient cause is notably lower due to the complexity of evaluating various counterfactual scenarios.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"contexts\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ground_truth\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 24,\n        \"samples\": [\n          \"The key dimensions of LLM trustworthiness that need to be considered when assessing alignment with human intentions are reliability, safety, fairness, resistance to misuse, explainability and reasoning, adherence to social norms, and robustness.\",\n          \"Aligned and unaligned LLMs perform differently in resisting misuse tasks. Aligned LLMs tend to follow instructions more closely, but they may still generate nonsensical or harmful outputs. Unaligned LLMs, on the other hand, are not intelligent enough to complete the task and often provide unrelated or nonsensical answers. Factors such as alignment to human instructions and trustworthiness contribute to their performance in resisting misuse tasks.\",\n          \"LLMs face challenges in understanding causality and performing causal reasoning tasks, such as inferring causal relationships among random variables and events, answering counterfactual questions, and understanding rules of structural causal models. They struggle with accurately inferring the sufficient causes of an event, as it requires considering all possible counterfactual scenarios with each event removed or replaced except the outcome and the possible sufficient cause event.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from phoenix.trace import using_project\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "def generate_response(query_engine, question):\n",
        "    response = query_engine.query(question)\n",
        "    return {\n",
        "        \"answer\": response.response,\n",
        "        \"contexts\": [c.node.get_content() for c in response.source_nodes],\n",
        "    }\n",
        "\n",
        "\n",
        "def generate_ragas_dataset(query_engine, test_df):\n",
        "    test_questions = test_df[\"question\"].values\n",
        "    responses = [generate_response(query_engine, q) for q in tqdm(test_questions)]\n",
        "\n",
        "    dataset_dict = {\n",
        "        \"question\": test_questions,\n",
        "        \"answer\": [response[\"answer\"] for response in responses],\n",
        "        \"contexts\": [response[\"contexts\"] for response in responses],\n",
        "        \"ground_truth\": test_df[\"ground_truth\"].values.tolist(),\n",
        "    }\n",
        "    ds = Dataset.from_dict(dataset_dict)\n",
        "    return ds\n",
        "\n",
        "\n",
        "with using_project(\"llama-index\"):\n",
        "    ragas_eval_dataset = generate_ragas_dataset(query_engine, test_df)\n",
        "\n",
        "ragas_evals_df = pd.DataFrame(ragas_eval_dataset)\n",
        "ragas_evals_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87117e89",
      "metadata": {
        "id": "87117e89"
      },
      "source": [
        "Check out Phoenix to view your LlamaIndex application traces."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4c15952d738946bd88b3b468ec1436f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8c6fb71d17054c8d8e0bc22d3d926dc1",
              "IPY_MODEL_394ebb8fbaa5476e83f353a7d4aaefd9",
              "IPY_MODEL_52a10b088e93442284c0c8f9d4606d65"
            ],
            "layout": "IPY_MODEL_4f531cfb0d76468f887c7387d8168da7"
          }
        },
        "8c6fb71d17054c8d8e0bc22d3d926dc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aba80e28377b4327981e6dbaf09c3341",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_09e684db46d048dba5347d1b23d1339c",
            "value": "embeddingâ€‡nodes:â€‡100%"
          }
        },
        "394ebb8fbaa5476e83f353a7d4aaefd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e86ffbee1cc245f78133364c2dc8d7fc",
            "max": 248,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bb1057f57ad54c1780e19b0c9afff1a6",
            "value": 248
          }
        },
        "52a10b088e93442284c0c8f9d4606d65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3deca45f82ad48e58c71d66dff5f3401",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2ff3442ea2e94421a01c2a4522c45654",
            "value": "â€‡247/248â€‡[00:13&lt;00:00,â€‡12.87it/s]"
          }
        },
        "4f531cfb0d76468f887c7387d8168da7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "aba80e28377b4327981e6dbaf09c3341": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09e684db46d048dba5347d1b23d1339c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e86ffbee1cc245f78133364c2dc8d7fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb1057f57ad54c1780e19b0c9afff1a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3deca45f82ad48e58c71d66dff5f3401": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ff3442ea2e94421a01c2a4522c45654": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b556140a342a407582ad1026ceb19b62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aa33477934124706b9a072eea805d976",
              "IPY_MODEL_c1413288bb22472eb56386908bd5b524",
              "IPY_MODEL_f6231b4102014f3888a1e4a6672194de"
            ],
            "layout": "IPY_MODEL_5d7057dbb7334aac926ce2d0e505b682"
          }
        },
        "aa33477934124706b9a072eea805d976": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5fe430c0bbdc4e74931e819db73c4e36",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_9d7fa201255c42d08f9cd398dbc89438",
            "value": "Generating:â€‡100%"
          }
        },
        "c1413288bb22472eb56386908bd5b524": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f56a89cd11e6459eb41f096bbb3e5b24",
            "max": 25,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8cbe98438aca440095da51d2ce8a691a",
            "value": 25
          }
        },
        "f6231b4102014f3888a1e4a6672194de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22b693505f714638a716b6c23da9f36e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_faf7f1dafd594adeb7da2adbc993af83",
            "value": "â€‡25/25â€‡[01:30&lt;00:00,â€‡â€‡8.24s/it]"
          }
        },
        "5d7057dbb7334aac926ce2d0e505b682": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fe430c0bbdc4e74931e819db73c4e36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d7fa201255c42d08f9cd398dbc89438": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f56a89cd11e6459eb41f096bbb3e5b24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cbe98438aca440095da51d2ce8a691a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "22b693505f714638a716b6c23da9f36e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "faf7f1dafd594adeb7da2adbc993af83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30e0edd0a05c4afc8ac50bcd19330410": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4398727f5388495a8718ab09c6c48955",
              "IPY_MODEL_4b4d447c9b81447e8d2aa447a80c8c16",
              "IPY_MODEL_00d7e60b2fe9437f81d0297f89230d60"
            ],
            "layout": "IPY_MODEL_6a3d11bcf73c4fd5a04cce38538a0b07"
          }
        },
        "4398727f5388495a8718ab09c6c48955": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08a04f1a14d9413ebb31ddc80868c44e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_cca53df0cdb946dba3fdc8e6e6ae43ce",
            "value": "100%"
          }
        },
        "4b4d447c9b81447e8d2aa447a80c8c16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e1c02e8fc344b19bebe527935a8c6e8",
            "max": 25,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f66180150e884a1e99c7cc4fa6141636",
            "value": 25
          }
        },
        "00d7e60b2fe9437f81d0297f89230d60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb569b6b4d3940d7a0247bfb980ac55f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_571f8ebc5f014469be4b12a87c7267f5",
            "value": "â€‡25/25â€‡[00:58&lt;00:00,â€‡â€‡2.79s/it]"
          }
        },
        "6a3d11bcf73c4fd5a04cce38538a0b07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08a04f1a14d9413ebb31ddc80868c44e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cca53df0cdb946dba3fdc8e6e6ae43ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4e1c02e8fc344b19bebe527935a8c6e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f66180150e884a1e99c7cc4fa6141636": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eb569b6b4d3940d7a0247bfb980ac55f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "571f8ebc5f014469be4b12a87c7267f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}